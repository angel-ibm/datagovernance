{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#data-governance-tutorial-api","title":"Data Governance Tutorial - API","text":"<p>Welcome to data governance with IBM Cloud Pak for Data and IBM Knowledge Catalog.</p> <p>Presentation video. Choose the tab of your preferred language. </p> DeutschEspa\u00f1olEnglish <p></p> <p></p> <p></p> <p>Unmute the sound with the control bar at the bottom of the video (speaker icon)</p> <p>Data Governance:  https://www.ibm.com/products/cloud-pak-for-data/governance</p> <p>IBM Knowledge Catalog: https://www.ibm.com/products/knowledge-catalog</p>"},{"location":"#whom-i-was-thinking-of-when-i-wrote-this-tutorial","title":"Whom I was thinking of when I wrote this tutorial?","text":"<p>Although I mainly thought of the technical guys at IBM and IBM Business Partners, the contents are valid and useable for everybody. It is true that some resources (the TechZone, to be more precise) are only available if you are in the close orbit of IBM but the code runs just on any Cloud Pak for Data environment if IKC has been properly deployed.</p> <p>I also thought of the professionals who need to support real life projects related to Data Governance. While most of the tutorials I visited are great for starters, there is no way to accomplish the massive tasks that a big project may demand without automating, at least, some of these tasks with an API. My goal was to ease the learning curve for those who are facing the challenge of programming with the Watson Data API for the first time.</p>"},{"location":"#how-to-use-this-tutorial","title":"How to use this tutorial","text":"<p>On the left bar, you will see the items corresponding to the suggested taks to get familiar with the Watson Data API, which is the main subject of the tutorial.</p> <p>First, the Basics section. It is intended to provide code for reviewing and understanding how the API works. Needless to say, they can be perfectly used in your own projects to perform the basic taks but I wrote them to be reviewed, so that is clear which steps are necessary and how to do them.</p> <p>Then, the Level 4 PoX section. This is an existing, very comprehensive tutorial written by IBM. It makes extensive use of the GUI to perform may data governance taks and, after running it completely it, it would be advisable to run this tutorial. It contains some of the PoX tasks, but implemented with the API. The intention is to compare what can be done with GUIs or with API and be able to decide in a real project which alternative is the best.</p> <p>Not all tasks or sections of the PoX are implemented here. The main reason is that some of them must be done supervised by a data steward (a human, no program) and it would make no sense to run it automatically.</p>"},{"location":"#requisites-for-this-tutorial","title":"Requisites for this tutorial","text":"<p>As stated in the introduction video, it is necessary to have a basic understanding of Data Governance (in general) and, more precisely, be familiar with the service IBM Knowledge Catalog contained in IBm Cloud Pak for Data.  </p> <p>Python programming is a must, no way to scape of it. Bash programming is optional, but highly recommended.</p> <p>The use of github repositories and jupyter notebooks are also a must. Otherwise, one can only read the contents. It is advisable to clone the repository on the own's laptop and execute all the tasks locally.</p> <p>In the PoX section, a way to execute notebooks from the git codespaces are briefly described. A browser would be enough to run everything of this tutorial (no need for a python environment, no local jupyter server, no code editor, not even a laptop!). Think of it as an experimental feature.</p> <p>Be prepared to adjust the locations of files and directories. The first lines of the jupyter notebooks will be a help (actually, a need) for customizing it to your environment. The good news is, setting at most three variables at the beginning would be enough.</p> <p>And,of course, you need a Cloud Pak for Data environment where IBM Knowledge Catalog is up and running. The code would work on any of the deployed options: on any cloud, Saas, PaaS, on premise or any vaariation of these.</p>"},{"location":"basic/","title":"1. Basics","text":""},{"location":"basic/#basic-tasks","title":"Basic Tasks","text":"<p>A graphical user interface is the way to get familiar with a vendor's data governance product line and even use it in production. However, the use of a programatic API like the Watson Data API, which is a REST interface,  can be very useful in the following situations:</p> <ul> <li>Automate common tasks or even one-time activities like backups and migrations</li> <li>Interface with other tools and programs that may use the REST interface</li> <li>Perform repetitive tasks involving a considerable amount of data</li> </ul> <p>In general, a REST API exposes a series of endpoints (URLs), each one intended to perform an individual task that can be customized providing the parameters prescribed in the documentation. This tutorial aims at easing the learning curve when trying to implement programs (actually, python scripts) that handle the calls to the most common methds of the Watson Data API, which involve not only to choose the right endpoint but also the expressing with the right syntax the intended parameters.</p> <p>The Watson Data API does not need to be installed explictitly. Anyone having access to a running Cloud Pak for Data deployment can use it, provided that he/she has the adequate privileges and, of course, the syntax is correct.</p>"},{"location":"basic/#how-to-use-these-snippets","title":"How to use these snippets","text":"<p>It is recommendable to copy the content of the snippets using the small copy icon on the top corner of the code cells. It will appear when you click on the gray zones. Then, you can paste the contents on your own files. Howeever, I also provide some the actual scripts I used for testing: they are either in the python or bash directories of the git repository.</p>"},{"location":"basic/#authentication-the-bearer-token","title":"Authentication -  The Bearer Token","text":"<p>Before attempting to use any of the methods of the API, we need to get authenticated by Cloud Pak for Data. We usually type our userid/password in the user interface, which is good for humans, but it is certainly not optimized for programs. That is why the Watson Data API make use of the so called \"Bearer Tokens\" to assert our identity every time we try to perform a task.</p> <p>Obtaining a bearer token and refreshing it when it expires are mandatory pre-requisites for all calls. The full process is explaned here </p> <p>Bearer tokens issued by the IBM Cloud expire after one hour. Remember to obtain a new one regularly</p> <p>The following snippets will generate a bearer token derived from the API key</p> PythonBash token.py<pre><code>import json\nimport requests\n\n# Read the API key from the file downloaded in the IBM Cloud\nf = open(\"wkcapikey.json\")\ndata = json.load(f)\napikey = data[\"apikey\"]\n\n# Get a bearer token with the API key\nurl = \"https://iam.cloud.ibm.com/identity/token\"\nheaders = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\ndata = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n\nr = requests.post(url, headers=headers, data=data)\naccess_token = r.json()[\"access_token\"]\n</code></pre> token.sh<pre><code>#!/bin/bash \n\n# Read the API key from the file downloaded in the IBM Cloud\n\nFILEAPIKEY=wkcapikey.json\napikey=$(jq -r .apikey $FILEAPIKEY)\n\n# Get a bearer token with the API key\n\nurl=\"https://iam.cloud.ibm.com/identity/token\"\nheader=\" -H Content-Type: application/x-www-form-urlencoded \"\nflags=\" -s -X POST\"\ndata=\" -d grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\"\ndata=${data}${apikey}\n\ntoken=$( \\\n    curl $flags $url $header $data \\\n    | jq -r .access_token)\n</code></pre> <p>If you are not working on the IBM Cloud, the endpoint of the call may be different. Look at the Level 4 PoX Section in this documentation and review the authentication cells provided in the notebooks.</p>"},{"location":"basic/#common-tasks","title":"Common Tasks","text":"<p>Provided that the issuer of the call has been granted with the proper rights, the following tasks can be easily performed using the API and will be exercised in this chapter (click on the REST Call to get mor information):</p> REST Call Description <code>GET /v2/projects</code> List the projects available for the issuer of the call in Cloud Pak for Data <code>GET /v2/catalogs</code> List the catalogs available in Cloud Pak for Data. Not only the list, but the specific information of one of them can be retrieved using parameter filtering <code>POST /v3/search</code> Search for any piece of information by using queries in Lucene or Elasticsearch syntax <code>GET /v3/governance_artifact_types/export</code> Export the full set of artifacts to a ZIP file <code>GET /v3/governance_artifact_types/{artifact_type}/export</code> Export just one kind of artifacts to a CSV file. The business terms (<code>glossary_term</code>) will be shown ind this chapter <code>POST /v3/governance_artifact_types/import</code> Import all artifacts from a ZIP file. <code>POST /v3/governance_artifact_types/{artifact_type}/import</code> Import just one kind of artifacts from a CSV file. The business terms (<code>glossary_term</code>) will be shown in this chapter <p>The following examples assume that the berarer token is already obtained, as described above. Note that the hostname part of the endpoint <code>api.eu-de.dataplatform.cloud.ibm.com</code> belongs to a system in the IBM Cloud.</p>"},{"location":"basic/#projects-and-catalogs","title":"Projects and Catalogs","text":"<p>This snippet obtains the name of all the projects and all the catalogs of the system.</p> <p>The number of projects and catalogs on a running system can be very large. Consider limiting the output </p> <p>Click on the appropriate tab to get the <code>bash</code> or <code>python</code> code. The highlighted lines mark the actual call to Cloud Pak for Data.</p> PythonBash projects_and_catalogs.py<pre><code>    # With the bearer token, we can issue requests \n\n    print(\"---- Projects ----\")\n\n    url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v2/projects\"\n    headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    r = requests.get(url, headers=headers)\n    for i in r.json()[\"resources\"] :\n        print(i[\"entity\"][\"name\"])\n\n    print(\"---- Catalogs ----\")\n\n    url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs\"\n    headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    r = requests.get(url, headers=headers)\n    for i in r.json()[\"catalogs\"] :\n        print(i[\"entity\"][\"name\"])\n</code></pre> projects_and_catalogs.sh<pre><code>    # With the bearer token, we can issue requests \n\n    echo \"---- Projects ----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/projects\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\"  \n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" | jq  '.resources[].entity.name'\n\n    echo \"---- Catalogs ----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\" \n\n    # all catalogs\n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" | jq '.catalogs[].entity.name'     \n</code></pre>"},{"location":"basic/#one-specific-item","title":"One specific item","text":"<p>This snippet retrieves one catalog, identified either by its name or by its <code>guid</code>. Note that the former is passed as parameter of the REST call and the latter as part of the endpoint address.</p> <p>The <code>for</code> loop is necessary in the python code is an iterable. Note that the bash code adresses it differently</p> <p>Click on the appropriate tab to get the <code>bash</code> or <code>python</code> code. The highlighted lines mark the actual call to Cloud Pak for Data or the important sentences.</p> PythonBash only_one_item.py<pre><code>        print(\"---- The catalog called Catalog-Angel ----\")\n        mycatalog=\"Catalog-Angel\"\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs?name=\" + mycatalog\n\n        r = requests.get(url, headers=headers)\n        for i in r.json()[\"catalogs\"] :\n            print(i[\"metadata\"][\"guid\"])\n            print(i[\"entity\"][\"name\"])\n\n        mycatalog_id = i[\"metadata\"][\"guid\"] \n        print(f\"---- The catalog with id = {mycatalog_id}  ----\")\n\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs/\" + mycatalog_id\n        r = requests.get(url, headers=headers)\n        print(r.json()[\"entity\"][\"name\"])\n</code></pre> only_one_item.sh<pre><code>        #  my catalog by name\n        CATALOG_NAME=\"Catalog-Angel\"\n        params=\"?name=${CATALOG_NAME}\"\n\n        echo ---- The catalog named $CATALOG_NAME ----\n\n        curl $flags $url$params $header \\\n           -H \"Authorization: Bearer ${token}\" | jq '.catalogs[] | [ .metadata.guid , .entity.name ]'\n\n        #  my catalog by id\n        mycatalog_id=$(curl $flags $url$params $header \\\n           -H \"Authorization: Bearer ${token}\" | jq -r '.catalogs[] | .metadata.guid ')\n\n        echo --- The catalog with id=$mycatalog_id ----\n\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs/$mycatalog_id\"\n\n    curl $flags $url $header \\\n   -H \"Authorization: Bearer ${token}\" | jq '.entity.name '     \n</code></pre>"},{"location":"basic/#search-capability","title":"Search Capability","text":"<p>There is an alternative way to retrieve an specific artifact (or a group of), which is the search call. It is fully described here. Instead of looking fo projects and catalogs as the previous snippets, the following examples search Categories and Business Terms (aka <code>glossary_term</code> for the API).</p> <p>The search syntax is known as Lucene query. Great for APIs\u2026 not so great for humans</p> <p>No bash code this time\u2026 it is clearer in python and I think this kind of queries may be too cumbersome for bash. The highlighted lines mark the lucene query syntax.</p> Pythonbash projects_and_catalogs.py<pre><code>        import json\n        import requests\n\n        # Read the API key from the file downloaded in the IBM Cloud\n        f = open(\"wkcapikey.json\")\n        data = json.load(f)\n        apikey = data[\"apikey\"]\n\n        # Get a bearer token with the API key\n        url = \"https://iam.cloud.ibm.com/identity/token\"\n        headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n\n        r = requests.post(url, headers=headers, data=data)\n        access_token = r.json()[\"access_token\"]\n\n        # With the bearer token, we can issue requests \n\n        print(\"---- All Categories ----\")\n\n        url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v3/search\"\n        headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n        data = '''{\n            \"query\": {\n                \"match\": {\n                    \"metadata.artifact_type\": \"category\"\n                }\n            }\n        }'''\n\n        r = requests.post(url, headers=headers, data=data)\n\n        for i in r.json()[\"rows\"] :\n            print(i[\"metadata\"][\"name\"])\n\n        print(\"---- Business Terms of MotoGP ----\")\n\n        data='''{\n            \"_source\":[ \"metadata.name\", \"metadata.description\"],\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\n                            \"match\": {\n                                \"metadata.artifact_type\": \"glossary_term\"\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"categories.primary_category_name\": \"MotoGP\"\n                            }\n                        }\n                    ]\n                }\n            }\n        }'''\n\n        r = requests.post(url, headers=headers, data=data)\n        for i in r.json()[\"rows\"] :\n            print(i[\"metadata\"][\"name\"], end=\": \")\n            print(i[\"metadata\"][\"description\"], end=\"\\n\\n\")\n</code></pre> <p>Too many fancy quotes, braces, escape backslashes\u2026 better in python</p>"},{"location":"basic/#export-artifacts","title":"Export Artifacts","text":"<p>This is one of the administrative tasks that everybody will do. It is about extracting all the information of the artifacts and export it to a file in order to keep a backup, transport them to another system, etc.</p> <p>Store all artifacts in a zip file</p> <p>No python code is provided. This task is much more likely to be done with bash. The highlited line indicates the output file.</p> bashpython export_all.sh<pre><code>    echo \"---- Export ----\"\n\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/export?include_custom_attribute_definitions=true\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\"  \n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" \\\n       -o governance_artifacts.zip\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p> <p>Export only the business terms to a csv file</p> <p>Another example: no zip file, but csv; not all artifacts, only business terms. Again, this task is much more likely to be performed in bash, not python. The highlited line indicates the output file. *</p> bashpython export.sh<pre><code>    echo \"---- Export only the business terms to CSV----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/glossary_term/export\"\n\n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" \\\n       -o business_terms.csv\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p>"},{"location":"basic/#import-artifacts","title":"Import Artifacts","text":"<p>The following examples are the counterparts of the export snippets. They are intended to ingest a large number of artifacts. <code>bash</code> is probably the best method to achive those tasks but, if you insist on doing with python, take a look at the jupyter notebooks on the Level 4 PoX section in this documentation.</p> <p>Import Business Terms</p> <p>The highlited line indicates the input file.</p> Bashpython import_business_terms.sh<pre><code>    echo \"---- Import business terms from CSV----\"\n\n    flags=' -X POST '\n    url=' https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/glossary_term/import?merge_option=all '\n    header=' -H Content-Type:multipart/form-data '\n\n    curl $flags $url $header \\\n       -H \"Authorization:Bearer ${token}\" \\\n       -F \"file=@\\\"mybusiness_terms.csv\\\";type=text/csv\"\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p> <p>Import all governance artifacts from a ZIP file</p> <p>The highlited line indicates the input file. Note that this import can take a long time and it is asynchronous, that is why a wait loop is included.</p> Bashpython import_all.sh<pre><code>    echo \"---- Import all artifact from a ZIP File ----\"\n\n    flags=' -s -X POST '\n    url=' https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/import?merge_option=specified '\n    header=' -H Content-Type:multipart/form-data '\n    import_process=$(curl $flags $url $header \\\n       -H \"Authorization:Bearer ${token}\" \\\n       -F \"file=@\\\"governance_artifacts.zip\\\"\"  \\\n       | jq -r .process_id)\n\n    echo \"----- Import process started: $import_process ----- \"\n\n    flags=' -s -X GET '\n    url=\" https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/import/status/${import_process} \"\n\n    while true\n    do\n        import_status=$(curl $flags $url \\\n        -H \"Authorization: Bearer ${token}\" \\\n        | jq -r .status)\n\n\n        if [ $import_status != \"IN_PROGRESS\" ]\n        then\n            break\n        fi\n\n        echo Import job in process. Please wait...\n        sleep 5\n    done\n\n    curl $flags $url \\\n        -H \"Authorization: Bearer ${token}\"\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p>"},{"location":"pox/","title":"2. Level 4 PoX","text":""},{"location":"pox/#level-4-pox-knowledge-catalog","title":"Level 4 PoX - Knowledge Catalog","text":"<p>IBM Data and AI Live Demos is a web site that contains detailed instructions, code repositories, scripts, and various other tools for IBM sellers and business partners for learning and getting hands-on practice with Cloud Pak for Data solutions.</p> <p> https://cp4d-outcomes.techzone.ibm.com</p> <p>From all available resources, this documentation enhances the  Level 4 PoX - Knowledge Catalog section by adding Jupyter notebooks to automate some tasks that may not be executed with the graphical user interface in real life projects like migrations, massive initial setup of governance artifacts, etc.</p> <p>Several notebooks are provided here \"as-is\" for learning purposes. They are intended to be adapted to  the specific needs of a project, a demonstration, etc. They can be reviewed in this web site, the cells can be copied-and-pasted individually and the full code can be downloaded for convenience. Additionally, they can be executed online using, for example, the github codespaces feature if the adequate Cloud Pak for Data environment has been provisioned.</p>"},{"location":"pox/#how-to-download-a-notebook-to-your-laptop","title":"How to download a notebook to your laptop","text":"<p>The blue sections below include a link for displaying the notebooks. If you click on them, the notebook will be shown and you will see the download icon on the top right corner. Just klick on it to start downloading.</p> <p></p> <p>Note that this link is just for reviewing and downloading the code. If you want to execute the notebook, you will need to store and run it from your laptop or in a proper enviroment like the one we will describe in the next section.</p>"},{"location":"pox/#how-to-run-a-notebook-online","title":"How to run a notebook online","text":"<p>The magenta sections below contain a link for accesing the notebooks in the original github repository. If you click on it, you will be directed to the right location on github.com. Go to the right corner, unfold the menu and select <code>github.dev</code> as shown in this picture:</p> <p></p> <p>Now, go again to the top right corner, click on <code>select kernel</code> and then click on <code>open in a codespace</code>:</p> <p></p> <p>Then, click on one of the options:</p> <p></p> <p>Finally, you can execute the notebook as it would be local in your laptop. Note that we incorporate this feature to the documentation just for convenience, only to try small things quickly.  </p>"},{"location":"pox/#show-me-how-to-run-those-notebooks-online","title":"Show me how to run those notebooks online","text":""},{"location":"pox/#sample-notebooks","title":"Sample Notebooks","text":""},{"location":"pox/#define-the-business-vocabulary","title":"Define the Business Vocabulary","text":"<p>This notebook automates all the tasks of the section <code>Define the Business Vocabulary</code></p> <p></p> <p>Display (and download) the notebook</p> <p>Click to display and download the notebook</p> <p>Access the original notebook (and execute it online) </p> <p>Click to access the actual file and execute it online</p>"},{"location":"pox/#define-the-rules-and-policies","title":"Define the Rules and Policies","text":"<p>This notebook automates all the tasks of the section <code>Define Rules and Policies</code></p> <p></p> <p>Display (and download) the notebook</p> <p>Click to display the notebook</p> <p>Access the original notebook (and execute it online) </p> <p>Click to access the actual file and execute it online</p>"},{"location":"pox/Define_Business_Vocabulary/","title":"Define Business Vocabulary","text":"<p>This notebook implements the section \"Define Business Vocabulary\" of the Level 4 PoX - Knowledge Catalog Tutorial and uses the Watson Data API</p> <p>Before executing the next cell, observe the variables <code>LOCAL_DIR_PREFIX</code> and <code>FILE_CREDENTIALS</code> and adjust them to your environment. If they are set wrong, the csv files containing the artifact definitions or your credentials in a file (in case you want to use one) will not be found.</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport requests # type: ignore\nimport time\n\nLOCAL_DIR_PREFIX = \"\"\n\n# Default: UNcomment the next line if you cloned the repository and the notebook will run locally on your laptop\n# Default: UNcomment the next line if it is running on a codespace (main address ending with github.dev)\n# COMMENT the next line if you are running on github.dev (main address starting with github.dev)\nLOCAL_DIR_PREFIX = \"../../\"\n\n# uncomment the next line if hardcoding the credentials on the code, global class variables\nFILE_CREDENTIALS = \"\"\n\n# uncomment the next two lines if using the file ikcapikey.json for storing credentials\n# FILE_CREDENTIALS = \"python/ikcapikey.json\"\n# FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS\n\nclass credentials :\n\n    file_credentials = \"\"\n    url_server = \"https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"\n    username = \"admin\"\n    apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"\n    access_token = \"\"\n\n    def __init__(self, file_credentials):\n\n        if file_credentials != \"\" :\n            try :\n                with open(file_credentials) as f :\n                    data = json.load(f)\n                    self.url_server = data[\"url_server\"]\n                    self.username = data[\"username\"]\n                    self.apikey = data[\"api_key\"]\n                    self.file_credentials = file_credentials\n            except :\n                print(\"Error with the file \", file_credentials)\n    \n   \n    def urlRequest(self, urlSuffix):\n        return self.url_server + urlSuffix\n\n    def get_bearer_token(self):\n        \n        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n        # url = \"https://iam.cloud.ibm.com/identity/token\"\n        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n        # r = requests.post(url, headers=headers, data=data)\n        # access_token = r.json()[\"access_token\"]\n\n        # Get a bearer token with the API key - Cloud Pak for Data Software\n        urlSuffix = \"/icp4d-api/v1/authorize\"\n        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n\n        if r.status_code != 200:\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        else :\n            try:\n                self.access_token = r.json()[\"token\"]\n            except KeyError:\n                print(\"Error with the token. Code: \", r.status_code)\n                print(\"Hint: check the credentials file \", self.file_credentials)\n                print(r.text)\n                \n            return self.access_token\n\nmyconn = credentials(FILE_CREDENTIALS)\naccess_token = myconn.get_bearer_token()\n</pre> import json import requests # type: ignore import time  LOCAL_DIR_PREFIX = \"\"  # Default: UNcomment the next line if you cloned the repository and the notebook will run locally on your laptop # Default: UNcomment the next line if it is running on a codespace (main address ending with github.dev) # COMMENT the next line if you are running on github.dev (main address starting with github.dev) LOCAL_DIR_PREFIX = \"../../\"  # uncomment the next line if hardcoding the credentials on the code, global class variables FILE_CREDENTIALS = \"\"  # uncomment the next two lines if using the file ikcapikey.json for storing credentials # FILE_CREDENTIALS = \"python/ikcapikey.json\" # FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS  class credentials :      file_credentials = \"\"     url_server = \"https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"     username = \"admin\"     apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"     access_token = \"\"      def __init__(self, file_credentials):          if file_credentials != \"\" :             try :                 with open(file_credentials) as f :                     data = json.load(f)                     self.url_server = data[\"url_server\"]                     self.username = data[\"username\"]                     self.apikey = data[\"api_key\"]                     self.file_credentials = file_credentials             except :                 print(\"Error with the file \", file_credentials)              def urlRequest(self, urlSuffix):         return self.url_server + urlSuffix      def get_bearer_token(self):                  # Get a bearer token with the API key - Cloud Pak for Data SaaS         # url = \"https://iam.cloud.ibm.com/identity/token\"         # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}         # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey         # r = requests.post(url, headers=headers, data=data)         # access_token = r.json()[\"access_token\"]          # Get a bearer token with the API key - Cloud Pak for Data Software         urlSuffix = \"/icp4d-api/v1/authorize\"         headers = {'Accept': 'application/json', 'Content-type': 'application/json'}         data = {\"username\" : self.username, \"api_key\" : self.apikey}         r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))          if r.status_code != 200:             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         else :             try:                 self.access_token = r.json()[\"token\"]             except KeyError:                 print(\"Error with the token. Code: \", r.status_code)                 print(\"Hint: check the credentials file \", self.file_credentials)                 print(r.text)                              return self.access_token  myconn = credentials(FILE_CREDENTIALS) access_token = myconn.get_bearer_token()  <p>Now, you can follow one by one the tasks as indicated in the PoX instructions.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Categories from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-categories.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/category/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n</pre> print(\"---- Import Categories from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-categories.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/category/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Update Classifications from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-classifications.csv\" \nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Update Classifications from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-classifications.csv\"  IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Data Classes from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-data-classes.csv\" \nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Data Classes from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-data-classes.csv\"  IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Business Terms from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-business-terms.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Business Terms from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-business-terms.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Reference Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-data.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Reference Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-data.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Load Department Lookup Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-department.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nartifact_id = None\nversion_id = None\nurlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200'\nheaders = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nif r.status_code == 200 :\n    for i in r.json()[\"resources\"] :\n        if i[\"name\"] == \"Department Lookup\" :\n            artifact_id = i[\"artifact_id\"]\n            version_id = i[\"version_id\"]\n            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n            break\nelse :\n    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n    print(r.text)\n\nif artifact_id is None or version_id is None:\n    print(\"Department Lookup not found\")\nelse :    \n    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n    headers = {\"Authorization\" : \"Bearer \" + access_token }\n    import_parameters = {\n        \"artifact_id_mode\": False,\n        \"code\": \"DEPARTMENT_CODE\",\n        \"first_row_header\": True,\n        \"import_relationships_only\": False,\n        \"skip_workflow_if_possible\": False, \n        \"trim_white_spaces\": True,\n        \"value\": \"DEPARTMENT_EN\",\n        \"value_conflicts\": \"OVERWRITE\" \n    }\n    files={\n        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n        'import_parameters' : (None, str(import_parameters))   \n    }\n    \n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\n    if r.status_code == 202 :\n        import_id = r.json()[\"import_info\"][\"import_id\"]\n        print(f\"----- Import process started: {import_id} ----- \")\n        print(\"----- Entering wait loop ------\")\n        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n        workflow_id = r.json()[\"workflow_id\"]\n\n        while True :\n            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n            if r.status_code != 200 :\n                print(\"Error with the request. Code: \", r.status_code)\n                print(r.text)\n                break\n            status = r.json()[\"import_info\"][\"import_state\"]\n            if status != \"IN_PROGRESS\" :\n                break\n            else :\n                print (\"Import in progess, please wait\")\n                time.sleep(5)\n        print(\"Import finished. Status = \", r.status_code)\n    else :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> print(\"---- Load Department Lookup Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-department.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  artifact_id = None version_id = None urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200' headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}  r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  if r.status_code == 200 :     for i in r.json()[\"resources\"] :         if i[\"name\"] == \"Department Lookup\" :             artifact_id = i[\"artifact_id\"]             version_id = i[\"version_id\"]             print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)             break else :     print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)     print(r.text)  if artifact_id is None or version_id is None:     print(\"Department Lookup not found\") else :         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'     headers = {\"Authorization\" : \"Bearer \" + access_token }     import_parameters = {         \"artifact_id_mode\": False,         \"code\": \"DEPARTMENT_CODE\",         \"first_row_header\": True,         \"import_relationships_only\": False,         \"skip_workflow_if_possible\": False,          \"trim_white_spaces\": True,         \"value\": \"DEPARTMENT_EN\",         \"value_conflicts\": \"OVERWRITE\"      }     files={         'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),         'import_parameters' : (None, str(import_parameters))        }          r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)      if r.status_code == 202 :         import_id = r.json()[\"import_info\"][\"import_id\"]         print(f\"----- Import process started: {import_id} ----- \")         print(\"----- Entering wait loop ------\")         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id         headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}          workflow_id = r.json()[\"workflow_id\"]          while True :             r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)             if r.status_code != 200 :                 print(\"Error with the request. Code: \", r.status_code)                 print(r.text)                 break             status = r.json()[\"import_info\"][\"import_state\"]             if status != \"IN_PROGRESS\" :                 break             else :                 print (\"Import in progess, please wait\")                 time.sleep(5)         print(\"Import finished. Status = \", r.status_code)     else :         print(\"Error with the request. Code: \", r.status_code)         print(r.text) In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Load Position Lookup Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-position.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nartifact_id = None\nversion_id = None\nurlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200'\nheaders = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nif r.status_code == 200 :\n    for i in r.json()[\"resources\"] :\n        if i[\"name\"] == \"Position Lookup\" :\n            artifact_id = i[\"artifact_id\"]\n            version_id = i[\"version_id\"]\n            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n            break\nelse :\n    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n    print(r.text)\n\nif artifact_id is None or version_id is None:\n    print(\"Position Lookup not found\")\nelse :    \n    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n    headers = {\"Authorization\" : \"Bearer \" + access_token }\n    import_parameters = {\n        \"artifact_id_mode\": False,\n        \"code\": \"POSITION_CODE\",\n        \"first_row_header\": True,\n        \"import_relationships_only\": False,\n        \"skip_workflow_if_possible\": False, \n        \"trim_white_spaces\": True,\n        \"value\": \"POSITION_EN\",\n        \"value_conflicts\": \"OVERWRITE\" \n    }\n    files={\n        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n        'import_parameters' : (None, str(import_parameters))   \n    }\n    \n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\n    if r.status_code == 202 :\n        import_id = r.json()[\"import_info\"][\"import_id\"]\n        print(f\"----- Import process started: {import_id} ----- \")\n        print(\"----- Entering wait loop ------\")\n        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n        workflow_id = r.json()[\"workflow_id\"]\n\n        while True :\n            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n            if r.status_code != 200 :\n                print(\"Error with the request. Code: \", r.status_code)\n                print(r.text)\n                break\n            status = r.json()[\"import_info\"][\"import_state\"]\n            if status != \"IN_PROGRESS\" :\n                break\n            else :\n                print (\"Import in progess, please wait\")\n                time.sleep(5)\n        print(\"Import finished. Status = \", r.status_code)\n    else :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre>  print(\"---- Load Position Lookup Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-position.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  artifact_id = None version_id = None urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200' headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}  r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  if r.status_code == 200 :     for i in r.json()[\"resources\"] :         if i[\"name\"] == \"Position Lookup\" :             artifact_id = i[\"artifact_id\"]             version_id = i[\"version_id\"]             print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)             break else :     print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)     print(r.text)  if artifact_id is None or version_id is None:     print(\"Position Lookup not found\") else :         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'     headers = {\"Authorization\" : \"Bearer \" + access_token }     import_parameters = {         \"artifact_id_mode\": False,         \"code\": \"POSITION_CODE\",         \"first_row_header\": True,         \"import_relationships_only\": False,         \"skip_workflow_if_possible\": False,          \"trim_white_spaces\": True,         \"value\": \"POSITION_EN\",         \"value_conflicts\": \"OVERWRITE\"      }     files={         'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),         'import_parameters' : (None, str(import_parameters))        }          r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)      if r.status_code == 202 :         import_id = r.json()[\"import_info\"][\"import_id\"]         print(f\"----- Import process started: {import_id} ----- \")         print(\"----- Entering wait loop ------\")         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id         headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}          workflow_id = r.json()[\"workflow_id\"]          while True :             r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)             if r.status_code != 200 :                 print(\"Error with the request. Code: \", r.status_code)                 print(r.text)                 break             status = r.json()[\"import_info\"][\"import_state\"]             if status != \"IN_PROGRESS\" :                 break             else :                 print (\"Import in progess, please wait\")                 time.sleep(5)         print(\"Import finished. Status = \", r.status_code)     else :         print(\"Error with the request. Code: \", r.status_code)         print(r.text) In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)"},{"location":"pox/Define_Business_Vocabulary/#define-business-vocabulary","title":"Define Business Vocabulary\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#authorisation-mandatory-customization","title":"Authorisation - mandatory customization\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#define-the-business-vocabulary","title":"Define the Business Vocabulary\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#1-create-categories","title":"1. Create Categories\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2-update-classifications","title":"2. Update Classifications\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2a-change-the-definitions","title":"2.a. Change the definitions\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2b-publish-the-definitions","title":"2.b. Publish the definitions\u00b6","text":"<p>Before executing this cell, you may want to check the \"Task Inbox\" in CloudPak for Data if you are not sure about what will be published</p>"},{"location":"pox/Define_Business_Vocabulary/#3-create-data-classes","title":"3. Create Data Classes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#3a-add-new-data-classes","title":"3.a. Add new Data Classes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#3b-publish-the-changes","title":"3.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4-create-business-terms","title":"4. Create Business Terms\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4a-add-new-business-terms","title":"4.a. Add new Business Terms\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4b-publish-the-changes","title":"4.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5-create-reference-data","title":"5. Create Reference Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5a-add-new-reference-data","title":"5.a Add new Reference Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5b-publish-the-changes","title":"5.b Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6-load-department-lookup-data","title":"6. Load Department Lookup Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6a-add-the-lookup-data","title":"6.a Add the lookup data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6b-publish-the-changes","title":"6.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7-load-position-lookup-data","title":"7. Load Position Lookup Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7a-add-the-new-data","title":"7.a. Add the new data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7b-publish-the-draft","title":"7.b. Publish the draft\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/","title":"Define Rules and Policies","text":"<p>This notebook implements the section \"Define Rules and Policies\" of the Level 4 PoX - Knowledge Catalog Tutorial and uses the Watson Data API</p> <p>Before executing the next cell, observe the variables <code>LOCAL_DIR_PREFIX</code> and <code>FILE_CREDENTIALS</code> and adjust them to your environment. If they are set wrong, the csv files containing the artifact definitions or your credentials in a file (in case you want to use one) will not be found.</p> In\u00a0[\u00a0]: Copied! <pre>import json\nimport requests # type: ignore\nimport time\n\nLOCAL_DIR_PREFIX = \"\"\n\n# Default: UNcomment the next line if you cloned the repository and the notebook will run locally on your LAPTOP\n# Default: UNcomment the next line if it is running on a CODESPACE (main address ending with github.dev)\n# COMMENT the next line if you are running on GITHUB.DEV (main address starting with github.dev)\nLOCAL_DIR_PREFIX = \"../../\"\n\n# uncomment the next line if hardcoding the credentials on the code, global class variables\nFILE_CREDENTIALS = \"\"\n\n# uncomment the next two lines if using the file ikcapikey.json for storing credentials\n# FILE_CREDENTIALS = \"python/ikcapikey.json\"\n# FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS\n\nclass credentials :\n\n    file_credentials = \"\"\n    url_server = \" https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"\n    username = \"admin\"\n    apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"\n    access_token = \"\"\n\n    def __init__(self, file_credentials):\n\n        if file_credentials != \"\" :\n            try :\n                with open(file_credentials) as f :\n                    data = json.load(f)\n                    self.url_server = data[\"url_server\"]\n                    self.username = data[\"username\"]\n                    self.apikey = data[\"api_key\"]\n                    self.file_credentials = file_credentials\n            except :\n                print(\"Error with the file \", file_credentials)\n    \n   \n    def urlRequest(self, urlSuffix):\n        return self.url_server + urlSuffix\n\n    def get_bearer_token(self):\n        \n        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n        # url = \"https://iam.cloud.ibm.com/identity/token\"\n        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n        # r = requests.post(url, headers=headers, data=data)\n        # access_token = r.json()[\"access_token\"]\n\n        # Get a bearer token with the API key - Cloud Pak for Data Software\n        urlSuffix = \"/icp4d-api/v1/authorize\"\n        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n\n        if r.status_code != 200:\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        else :\n            try:\n                self.access_token = r.json()[\"token\"]\n            except KeyError:\n                print(\"Error with the token. Code: \", r.status_code)\n                print(\"Hint: check the credentials file \", self.file_credentials)\n                print(r.text)\n                \n            return self.access_token\n\nmyconn = credentials(FILE_CREDENTIALS)\naccess_token = myconn.get_bearer_token()\n</pre> import json import requests # type: ignore import time  LOCAL_DIR_PREFIX = \"\"  # Default: UNcomment the next line if you cloned the repository and the notebook will run locally on your LAPTOP # Default: UNcomment the next line if it is running on a CODESPACE (main address ending with github.dev) # COMMENT the next line if you are running on GITHUB.DEV (main address starting with github.dev) LOCAL_DIR_PREFIX = \"../../\"  # uncomment the next line if hardcoding the credentials on the code, global class variables FILE_CREDENTIALS = \"\"  # uncomment the next two lines if using the file ikcapikey.json for storing credentials # FILE_CREDENTIALS = \"python/ikcapikey.json\" # FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS  class credentials :      file_credentials = \"\"     url_server = \" https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"     username = \"admin\"     apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"     access_token = \"\"      def __init__(self, file_credentials):          if file_credentials != \"\" :             try :                 with open(file_credentials) as f :                     data = json.load(f)                     self.url_server = data[\"url_server\"]                     self.username = data[\"username\"]                     self.apikey = data[\"api_key\"]                     self.file_credentials = file_credentials             except :                 print(\"Error with the file \", file_credentials)              def urlRequest(self, urlSuffix):         return self.url_server + urlSuffix      def get_bearer_token(self):                  # Get a bearer token with the API key - Cloud Pak for Data SaaS         # url = \"https://iam.cloud.ibm.com/identity/token\"         # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}         # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey         # r = requests.post(url, headers=headers, data=data)         # access_token = r.json()[\"access_token\"]          # Get a bearer token with the API key - Cloud Pak for Data Software         urlSuffix = \"/icp4d-api/v1/authorize\"         headers = {'Accept': 'application/json', 'Content-type': 'application/json'}         data = {\"username\" : self.username, \"api_key\" : self.apikey}         r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))          if r.status_code != 200:             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         else :             try:                 self.access_token = r.json()[\"token\"]             except KeyError:                 print(\"Error with the token. Code: \", r.status_code)                 print(\"Hint: check the credentials file \", self.file_credentials)                 print(r.text)                              return self.access_token  myconn = credentials(FILE_CREDENTIALS) access_token = myconn.get_bearer_token()  <p>Now, you can follow one by one the tasks as indicated in the PoX instructions</p> <p>1.a. Add new rules</p> In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Governance Rules from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-rules.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/rule/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Import Governance Rules from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-rules.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/rule/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Email Address\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"Email Address\"\ndata_class_name_for_search = \"Email ?Address\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nelse : \n\n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {\n                \"name\": \"Protect Email Address\",\n                \"description\": \"Protect all email addresses using the data privacy advanced masking method.\",\n                \"governance_type_id\": \"Access\",\n                \"trigger\": [\n                    \"$Asset.InferredClassification\",\n                    \"CONTAINS\",\n                    [\n                        '$' + global_id\n                    ]\n                ],\n                \"action\": {\n                    \"name\": \"Transform\",\n                    \"subaction\": {\n                        \"name\": \"pseudonymizeTerms\",\n                        \"parameters\": [\n                            {\n                                \"name\": \"term_name\",\n                                \"value\": global_id\n                            },\n                            {\n                                \"name\": \"maskingType\",\n                                \"value\": \"Partial\"\n                            },\n                            {\n                                \"name\": \"maskingProcessor\",\n                                \"value\": \"RepeatableFormatFabrication\"\n                            },\n                            {\n                                \"name\": \"preserveFormat\",\n                                \"value\": \"true\"\n                            },\n                            {\n                                \"name\": \"maskingOptions\",\n                                \"value\": [\n                                    {\n                                        \"name\": \"User name\",\n                                        \"value\": \"Generate user name\"\n                                    },\n                                    {\n                                        \"name\": \"Domain name\",\n                                        \"value\": \"Original\"\n                                    }\n                                ]\n                            }\n                        ]\n                    }\n                },\n                \"state\": \"active\"\n            }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Email Address\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"Email Address\" data_class_name_for_search = \"Email ?Address\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  else :       for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                 \"name\": \"Protect Email Address\",                 \"description\": \"Protect all email addresses using the data privacy advanced masking method.\",                 \"governance_type_id\": \"Access\",                 \"trigger\": [                     \"$Asset.InferredClassification\",                     \"CONTAINS\",                     [                         '$' + global_id                     ]                 ],                 \"action\": {                     \"name\": \"Transform\",                     \"subaction\": {                         \"name\": \"pseudonymizeTerms\",                         \"parameters\": [                             {                                 \"name\": \"term_name\",                                 \"value\": global_id                             },                             {                                 \"name\": \"maskingType\",                                 \"value\": \"Partial\"                             },                             {                                 \"name\": \"maskingProcessor\",                                 \"value\": \"RepeatableFormatFabrication\"                             },                             {                                 \"name\": \"preserveFormat\",                                 \"value\": \"true\"                             },                             {                                 \"name\": \"maskingOptions\",                                 \"value\": [                                     {                                         \"name\": \"User name\",                                         \"value\": \"Generate user name\"                                     },                                     {                                         \"name\": \"Domain name\",                                         \"value\": \"Original\"                                     }                                 ]                             }                         ]                     }                 },                 \"state\": \"active\"             }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Phone Number\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"Phone Number\"\ndata_class_name_for_search = \"Phone ?Number\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\nelse : \n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {            \n            \"name\": \"Protect Phone Numbers\",\n            \"description\": \"Protect all phone numbers using the redaction data privacy masking method.\",\n            \"governance_type_id\": \"Access\",\n            \"trigger\": [\n                \"$Asset.InferredClassification\",\n                \"CONTAINS\",\n                [\n                    '$' + global_id\n                ]\n            ],\n            \"action\": {\n                \"name\": \"Transform\",\n                \"subaction\": {\n                    \"name\": \"redactTerms\",\n                    \"parameters\": [\n                        {\n                            \"name\": \"term_name\",\n                            \"value\": global_id\n                        },\n                        {\n                            \"name\": \"maskingChar\",\n                            \"value\": \"X\"\n                        },\n                        {\n                            \"name\": \"maskingType\",\n                            \"value\": \"Full\"\n                        },\n                        {\n                            \"name\": \"preserveFormat\",\n                            \"value\": \"true\"\n                        }\n                    ]\n                }\n            },\n            \"state\": \"active\"\n        }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Phone Number\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"Phone Number\" data_class_name_for_search = \"Phone ?Number\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text) else :      for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                         \"name\": \"Protect Phone Numbers\",             \"description\": \"Protect all phone numbers using the redaction data privacy masking method.\",             \"governance_type_id\": \"Access\",             \"trigger\": [                 \"$Asset.InferredClassification\",                 \"CONTAINS\",                 [                     '$' + global_id                 ]             ],             \"action\": {                 \"name\": \"Transform\",                 \"subaction\": {                     \"name\": \"redactTerms\",                     \"parameters\": [                         {                             \"name\": \"term_name\",                             \"value\": global_id                         },                         {                             \"name\": \"maskingChar\",                             \"value\": \"X\"                         },                         {                             \"name\": \"maskingType\",                             \"value\": \"Full\"                         },                         {                             \"name\": \"preserveFormat\",                             \"value\": \"true\"                         }                     ]                 }             },             \"state\": \"active\"         }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Phone Number\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"US Social Security Number\"\ndata_class_name_for_search = \"US ?Social ?Security ?Number\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\nelse : \n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {            \n           \"name\": \"Protect US Social Security Numbers\",\n            \"description\": \"Protect all US social security numbers using the data privacy advanced masking method.\",\n            \"governance_type_id\": \"Access\",\n            \"trigger\": [\n                \"$Asset.InferredClassification\",\n                \"CONTAINS\",\n                [\n                    \"$\" +  global_id\n                ]\n            ],\n            \"action\": {\n                \"name\": \"Transform\",\n                \"subaction\": {\n                    \"name\": \"pseudonymizeTerms\",\n                    \"parameters\": [\n                        {\n                            \"name\": \"term_name\",\n                            \"value\": global_id\n                        },\n                        {\n                            \"name\": \"maskingType\",\n                            \"value\": \"Full\"\n                        },\n                        {\n                            \"name\": \"maskingProcessor\",\n                            \"value\": \"FormatPreservingTokenization\"\n                        },\n                        {\n                            \"name\": \"preserveFormat\",\n                            \"value\": \"true\"\n                        }\n                    ]\n                }\n            },\n            \"state\": \"active\"\n        }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Phone Number\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"US Social Security Number\" data_class_name_for_search = \"US ?Social ?Security ?Number\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text) else :      for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                        \"name\": \"Protect US Social Security Numbers\",             \"description\": \"Protect all US social security numbers using the data privacy advanced masking method.\",             \"governance_type_id\": \"Access\",             \"trigger\": [                 \"$Asset.InferredClassification\",                 \"CONTAINS\",                 [                     \"$\" +  global_id                 ]             ],             \"action\": {                 \"name\": \"Transform\",                 \"subaction\": {                     \"name\": \"pseudonymizeTerms\",                     \"parameters\": [                         {                             \"name\": \"term_name\",                             \"value\": global_id                         },                         {                             \"name\": \"maskingType\",                             \"value\": \"Full\"                         },                         {                             \"name\": \"maskingProcessor\",                             \"value\": \"FormatPreservingTokenization\"                         },                         {                             \"name\": \"preserveFormat\",                             \"value\": \"true\"                         }                     ]                 }             },             \"state\": \"active\"         }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Policies from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-policies.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/policy/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Import Policies from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-policies.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/policy/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)"},{"location":"pox/Define_Rules_and_Policies/#define-rules-and-policies","title":"Define Rules and Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#authorisation-mandatory-customization","title":"Authorisation - mandatory customization\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#define-rules-and-policies","title":"Define Rules and Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#1-create-governance-rules","title":"1. Create Governance Rules\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#1b-publish-the-changes","title":"1.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#2-create-email-protection-rule","title":"2. Create Email Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#3-create-phone-protection-rule","title":"3. Create Phone Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#4-create-us-ssn-protection-rule","title":"4. Create US SSN Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5-create-policies","title":"5. Create Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5a-add-new-policies","title":"5.a. Add new Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5b-publish-the-changes","title":"5.b. Publish the changes\u00b6","text":""}]}