





import json
import requests # type: ignore
import time

class credentials :

    file_credentials = ""
    url_server = "https://cpd-cp4d.apps.6522a8d3368cb40016de15e7.cloud.techzone.ibm.com"
    username = "angelito"
    apikey = "c6n71XNLAfimgf9chAXe4ThEaYvXWf3oV7AIwhh6"
    access_token = ""

    def __init__(self, file_credentials):

        if file_credentials != "" :
            try :
                with open(file_credentials) as f :
                    data = json.load(f)
                    self.url_server = data["url_server"]
                    self.username = data["username"]
                    self.apikey = data["api_key"]
                    self.file_credentials = file_credentials
            except :
                print("Error with the file ", file_credentials)
    
   
    def urlRequest(self, urlSuffix):
        return self.url_server + urlSuffix

    def get_bearer_token(self):
        
        # Get a bearer token with the API key - Cloud Pak for Data SaaS
        # url = "https://iam.cloud.ibm.com/identity/token"
        # headers = {"Content-Type" : "application/x-www-form-urlencoded"}
        # data = "grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey=" + apikey
        # r = requests.post(url, headers=headers, data=data)
        # access_token = r.json()["access_token"]

        # Get a bearer token with the API key - Cloud Pak for Data Software
        urlSuffix = "/icp4d-api/v1/authorize"
        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}
        data = {"username" : self.username, "api_key" : self.apikey}
        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))

        if r.status_code != 200:
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
        else :
            try:
                self.access_token = r.json()["token"]
            except KeyError:
                print("Error with the token. Code: ", r.status_code)
                print("Hint: check the credentials file ", self.file_credentials)
                print(r.text)
                
            return self.access_token
    
FILE_CREDENTIALS = ""

myconn = credentials(FILE_CREDENTIALS)
access_token = myconn.get_bearer_token()









print("---- Import Categories from CSV----")

IMPORT_CSV_FILE = "governance-categories.csv"
urlSuffix='/v3/governance_artifact_types/category/import?merge_option=all'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}

r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

if r.status_code == 200 :
    status = r.json()["status"]
    print("Import finished. Status = ", status)
    print(r.text)
elif r.status_code == 202 :
    process_id = r.json()["process_id"]
    print(f"----- Import process started: {process_id} ----- ")
    print("----- Entering wait loop ------")
    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id
    headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
    while True :
        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
        if r.status_code != 200 :
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
        status = r.json()["status"]
        if status != "IN_PROGRESS" :
            break
        else :
            print ("Import in progess, please wait")
            time.sleep(5)
else :
    print("Error with the request. Code: ", r.status_code)
    print(r.text)









print("---- Update Classifications from CSV----")

IMPORT_CSV_FILE = "governance-classifications.csv" 
urlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}

r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

if r.status_code == 200 :
    status = r.json()["status"]
    print("Import finished. Status = ", status)
    print(r.text)

elif r.status_code == 202 :
    process_id = r.json()["process_id"]
    print(f"----- Import process started: {process_id} ----- ")
    print("----- Entering wait loop ------")
    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id
    headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

    while True :
        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
        if r.status_code != 200 :
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
            break
        status = r.json()["status"]
        if status != "IN_PROGRESS" :
            break
        else :
            print ("Import in progess, please wait")
            time.sleep(5)
else :
    print("Error with the request. Code: ", r.status_code)
    print(r.text)

workflow_id = r.json()["workflow_id"]






urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)








print("---- Create Data Classes from CSV----")

IMPORT_CSV_FILE = "governance-data-classes.csv" 
urlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}

r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

if r.status_code == 200 :
    status = r.json()["status"]
    print("Import finished. Status = ", status)
    print(r.text)

elif r.status_code == 202 :
    process_id = r.json()["process_id"]
    print(f"----- Import process started: {process_id} ----- ")
    print("----- Entering wait loop ------")
    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id
    headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

    while True :
        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
        if r.status_code != 200 :
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
            break
        status = r.json()["status"]
        if status != "IN_PROGRESS" :
            break
        else :
            print ("Import in progess, please wait")
            time.sleep(5)
else :
    print("Error with the request. Code: ", r.status_code)
    print(r.text)

workflow_id = r.json()["workflow_id"]





urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)









print("---- Create Business Terms from CSV----")

IMPORT_CSV_FILE = "governance-business-terms.csv"
urlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}

r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

if r.status_code == 200 :
    status = r.json()["status"]
    print("Import finished. Status = ", status)

elif r.status_code == 202 :
    process_id = r.json()["process_id"]
    print(f"----- Import process started: {process_id} ----- ")
    print("----- Entering wait loop ------")
    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id
    headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

    while True :
        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
        if r.status_code != 200 :
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
            break
        status = r.json()["status"]
        if status != "IN_PROGRESS" :
            break
        else :
            print ("Import in progess, please wait")
            time.sleep(5)
else :
    print("Error with the request. Code: ", r.status_code)
    print(r.text)

workflow_id = r.json()["workflow_id"]





urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)









print("---- Create Reference Data from CSV----")

IMPORT_CSV_FILE = "governance-reference-data.csv"
urlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}

r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

if r.status_code == 200 :
    status = r.json()["status"]
    print("Import finished. Status = ", status)

elif r.status_code == 202 :
    process_id = r.json()["process_id"]
    print(f"----- Import process started: {process_id} ----- ")
    print("----- Entering wait loop ------")
    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id
    headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

    while True :
        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
        if r.status_code != 200 :
            print("Error with the request. Code: ", r.status_code)
            print(r.text)
            break
        status = r.json()["status"]
        if status != "IN_PROGRESS" :
            break
        else :
            print ("Import in progess, please wait")
            time.sleep(5)
else :
    print("Error with the request. Code: ", r.status_code)
    print(r.text)

workflow_id = r.json()["workflow_id"]





urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)








print("---- Load Department Lookup Data from CSV----")

IMPORT_CSV_FILE = "governance-reference-department.csv"

artifact_id = None
version_id = None
urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'
headers = {"accept": "application/json" ,"Authorization" : "Bearer " + access_token}

r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

if r.status_code == 200 :
    for i in r.json()["resources"] :
        if i["name"] == "Department Lookup" :
            artifact_id = i["artifact_id"]
            version_id = i["version_id"]
            print("artifact_id = ", artifact_id, " version_id = " , version_id)
            break
else :
    print("Error in retrieving reference data artifacts, Code = ", r.status_code)
    print(r.text)

if artifact_id is None or version_id is None:
    print("Department Lookup not found")
else :    
    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'
    headers = {"Authorization" : "Bearer " + access_token }
    import_parameters = {
        "artifact_id_mode": False,
        "code": "DEPARTMENT_CODE",
        "first_row_header": True,
        "import_relationships_only": False,
        "skip_workflow_if_possible": False, 
        "trim_white_spaces": True,
        "value": "DEPARTMENT_EN",
        "value_conflicts": "OVERWRITE" 
    }
    files={
        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),
        'import_parameters' : (None, str(import_parameters))   
    }
    
    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

    if r.status_code == 202 :
        import_id = r.json()["import_info"]["import_id"]
        print(f"----- Import process started: {import_id} ----- ")
        print("----- Entering wait loop ------")
        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id
        headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

        workflow_id = r.json()["workflow_id"]

        while True :
            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
            if r.status_code != 200 :
                print("Error with the request. Code: ", r.status_code)
                print(r.text)
                break
            status = r.json()["import_info"]["import_state"]
            if status != "IN_PROGRESS" :
                break
            else :
                print ("Import in progess, please wait")
                time.sleep(5)
        print("Import finished. Status = ", r.status_code)
    else :
        print("Error with the request. Code: ", r.status_code)
        print(r.text)





urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)









print("---- Load Position Lookup Data from CSV----")

IMPORT_CSV_FILE = "governance-reference-position.csv"

artifact_id = None
version_id = None
urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'
headers = {"accept": "application/json" ,"Authorization" : "Bearer " + access_token}

r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

if r.status_code == 200 :
    for i in r.json()["resources"] :
        if i["name"] == "Position Lookup" :
            artifact_id = i["artifact_id"]
            version_id = i["version_id"]
            print("artifact_id = ", artifact_id, " version_id = " , version_id)
            break
else :
    print("Error in retrieving reference data artifacts, Code = ", r.status_code)
    print(r.text)

if artifact_id is None or version_id is None:
    print("Position Lookup not found")
else :    
    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'
    headers = {"Authorization" : "Bearer " + access_token }
    import_parameters = {
        "artifact_id_mode": False,
        "code": "POSITION_CODE",
        "first_row_header": True,
        "import_relationships_only": False,
        "skip_workflow_if_possible": False, 
        "trim_white_spaces": True,
        "value": "POSITION_EN",
        "value_conflicts": "OVERWRITE" 
    }
    files={
        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),
        'import_parameters' : (None, str(import_parameters))   
    }
    
    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)

    if r.status_code == 202 :
        import_id = r.json()["import_info"]["import_id"]
        print(f"----- Import process started: {import_id} ----- ")
        print("----- Entering wait loop ------")
        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id
        headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}

        workflow_id = r.json()["workflow_id"]

        while True :
            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)
            if r.status_code != 200 :
                print("Error with the request. Code: ", r.status_code)
                print(r.text)
                break
            status = r.json()["import_info"]["import_state"]
            if status != "IN_PROGRESS" :
                break
            else :
                print ("Import in progess, please wait")
                time.sleep(5)
        print("Import finished. Status = ", r.status_code)
    else :
        print("Error with the request. Code: ", r.status_code)
        print(r.text)





urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)

user_tasks = r.json()["entity"]["user_tasks"]
for i in user_tasks :
    if i["metadata"]["workflow_id"] == workflow_id :
        task_id = i["metadata"]["task_id"]

urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'
headers = {"accept": "application/json", "Authorization" : "Bearer " + access_token}
payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}
r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)

if r.status_code == 202 or r.status_code == 204 :
    print("Publish Successful, Code = ", r.status_code)
else :
    print("Error in publishing artifacts, Code = ", r.status_code)
    print(r.text)
