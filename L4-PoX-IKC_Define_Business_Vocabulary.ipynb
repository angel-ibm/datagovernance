{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API - Level 4 PoX - IBM Knowledge Catalog\n",
    "\n",
    "https://cloud.ibm.com/apidocs/watson-data-api-cpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests # type: ignore\n",
    "import time\n",
    "\n",
    "class credentials :\n",
    "\n",
    "    file_credentials = \"\"\n",
    "    url_server = \"\"\n",
    "    username = \"\"\n",
    "    apikey = \"\"\n",
    "    access_token = \"\"\n",
    "\n",
    "    def __init__(self, file_credentials):\n",
    "        try :\n",
    "            with open(file_credentials) as f :\n",
    "                data = json.load(f)\n",
    "                self.url_server = data[\"url_server\"]\n",
    "                self.username = data[\"username\"]\n",
    "                self.apikey = data[\"api_key\"]\n",
    "                self.file_credentials = file_credentials\n",
    "        except :\n",
    "            print(\"Error with the file \", file_credentials)\n",
    "    \n",
    "   \n",
    "    def urlRequest(self, urlSuffix):\n",
    "        return self.url_server + urlSuffix\n",
    "\n",
    "    def get_bearer_token(self):\n",
    "        \n",
    "        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n",
    "        # url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n",
    "        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey=\" + apikey\n",
    "        # r = requests.post(url, headers=headers, data=data)\n",
    "        # access_token = r.json()[\"access_token\"]\n",
    "\n",
    "        # Get a bearer token with the API key - Cloud Pak for Data Software\n",
    "        urlSuffix = \"/icp4d-api/v1/authorize\"\n",
    "        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n",
    "        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n",
    "        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "        else :\n",
    "            try:\n",
    "                self.access_token = r.json()[\"token\"]\n",
    "            except KeyError:\n",
    "                print(\"Error with the token. Code: \", r.status_code)\n",
    "                print(\"Hint: check the credentials file \", self.file_credentials)\n",
    "                print(r.text)\n",
    "                \n",
    "            return self.access_token\n",
    "    \n",
    "FILE_CREDENTIALS = \"ikcapikey.json\"\n",
    "\n",
    "myconn = credentials(FILE_CREDENTIALS)\n",
    "access_token = myconn.get_bearer_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Business Vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Categories\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- Import Categories from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-categories.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/category/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Update Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a. Change the definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Update Classifications from CSV----\n",
      "Import finished. Status =  SUCCEEDED\n",
      "{\n",
      "  \"process_id\": \"573630ed-b047-4fba-96e5-884ef8c90a9a\",\n",
      "  \"status\": \"SUCCEEDED\",\n",
      "  \"step_number\": 16,\n",
      "  \"total_steps\": 16,\n",
      "  \"step_message\": \"Artifacts import finished\",\n",
      "  \"creation_time\": \"2024-04-17T08:01:51.376Z\",\n",
      "  \"completion_time\": \"2024-04-17T08:01:57.949Z\",\n",
      "  \"tenant_id\": \"999\",\n",
      "  \"workflow_id\": \"c09982fd-fc90-11ee-9527-0a580a820487\",\n",
      "  \"heartbeat_time\": \"2024-04-17T08:01:57.949Z\",\n",
      "  \"messages\": {\n",
      "    \"resources\": [],\n",
      "    \"offset\": 0,\n",
      "    \"set_uri\": false,\n",
      "    \"limit\": 200,\n",
      "    \"count\": 0\n",
      "  },\n",
      "  \"operations_count\": {\n",
      "    \"relationship\": {\n",
      "      \"IMPORT_CREATE\": 8\n",
      "    },\n",
      "    \"classification\": {\n",
      "      \"IMPORT_CREATE\": 0,\n",
      "      \"IMPORT_MODIFY\": 4\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Update Classifications from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-classifications.csv\" \n",
    "urlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b. Publish the definitions\n",
    "\n",
    "Before executing this cell, you may want to check the \"Task Inbox\" in CloudPak for Data if you are not sure about what will be published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"metadata\":{\"name\":\"{\\\"id\\\":\\\"wkc-governance-workflows.default.workflowName.IMPORT.many\\\", \\\"defaultMessage\\\":\\\"{draft_mode} {artifactType} {artifactName}\\\", \\\"§artifactType\\\":\\\"wkc-governance-workflows.default.artifactType.classification\\\", \\\"§artifactTypePlural\\\":\\\"wkc-governance-workflows.default.artifactType.classification.plural\\\", \\\"artifactName\\\":\\\"Confidential\\\",\\\"§draft_mode\\\":\\\"wkc-governance-workflows.default.triggerProperty.event.IMPORT\\\"}\",\"state\":\"running\",\"artifact_type\":\"workflow\",\"workflow_id\":\"c09982fd-fc90-11ee-9527-0a580a820487\",\"configuration_id\":\"d6d1d2c9-0a54-4a45-a6ba-ba3daad330ee\",\"created_by\":\"1000330999\",\"created_at\":\"2024-04-17T08:01:53.184Z\"},\"entity\":{\"artifacts\":[{\"metadata\":{\"name\":\"Confidential\",\"artifact_type\":\"classification\",\"artifact_id\":\"d462e63f-b0ff-49f9-887c-6b6cad752404\",\"version_id\":\"a6ea9ccf-2a22-4d9d-9349-16cb2030c951\",\"published_ancestor_id\":\"f5042f76-7a74-4d3d-9a5c-1703c7ea89fa_0\",\"primary_category\":{\"id\":\"e39ada11-8338-3704-90e3-681a71e7c839\",\"name\":\"[uncategorized]\"}},\"workflow_data\":{\"base_url\":\"https://internal-nginx-svc.cpd.svc:12443/v3/classifications\",\"draft_mode\":\"import_modified\"}},{\"metadata\":{\"name\":\"Personal Information\",\"artifact_type\":\"classification\",\"artifact_id\":\"b3d1664c-daa9-4018-a30c-9bff1bd8cc47\",\"version_id\":\"94fa84c2-5e57-488a-831f-fcfd8e742cd3\",\"published_ancestor_id\":\"00b36150-5b02-41e1-b0ce-0264721504f7_0\",\"primary_category\":{\"id\":\"e39ada11-8338-3704-90e3-681a71e7c839\",\"name\":\"[uncategorized]\"}},\"workflow_data\":{\"base_url\":\"https://internal-nginx-svc.cpd.svc:12443/v3/classifications\",\"draft_mode\":\"import_modified\"}},{\"metadata\":{\"name\":\"Personally Identifiable Information\",\"artifact_type\":\"classification\",\"artifact_id\":\"4b29904f-de6b-4180-b2d8-112818e28b39\",\"version_id\":\"819f9d21-fea7-4524-b7a6-810b37cb9ed1\",\"published_ancestor_id\":\"eebfa107-b122-4b70-aa29-c992043450a0_0\",\"primary_category\":{\"id\":\"e39ada11-8338-3704-90e3-681a71e7c839\",\"name\":\"[uncategorized]\"}},\"workflow_data\":{\"base_url\":\"https://internal-nginx-svc.cpd.svc:12443/v3/classifications\",\"draft_mode\":\"import_modified\"}},{\"metadata\":{\"name\":\"Sensitive Personal Information\",\"artifact_type\":\"classification\",\"artifact_id\":\"a51709d4-436d-498f-96d8-6e43862bfa73\",\"version_id\":\"e12af10b-2ffb-4e79-b707-605825f13605\",\"published_ancestor_id\":\"718874f1-e84a-4818-b21c-67e0dfff2847_0\",\"primary_category\":{\"id\":\"e39ada11-8338-3704-90e3-681a71e7c839\",\"name\":\"[uncategorized]\"}},\"workflow_data\":{\"base_url\":\"https://internal-nginx-svc.cpd.svc:12443/v3/classifications\",\"draft_mode\":\"import_modified\"}}],\"workflow_template_id\":\"autoPublish:1:e89aa47d-fbf2-11ee-9527-0a580a820487\",\"workflow_template_key\":\"autoPublish\",\"workflow_template_version\":2,\"user_tasks\":[{\"metadata\":{\"name\":\"aaAuthoring\",\"state\":\"created\",\"artifact_type\":\"user_task\",\"task_id\":\"c22ea12a-fc90-11ee-9527-0a580a820487\",\"created_at\":\"2024-04-17T08:01:55.835Z\",\"workflow_id\":\"c09982fd-fc90-11ee-9527-0a580a820487\",\"workflow_type_id\":\"governance_artifacts\"},\"entity\":{\"priority\":50,\"task_title\":\"{\\\"id\\\":\\\"wkc-governance-workflows.autoPublish.authoring.taskTitle.IMPORT.many\\\", \\\"defaultMessage\\\":\\\"Publish {artifactTypePlural}\\\", \\\"§artifactType\\\":\\\"wkc-governance-workflows.default.artifactType.classification\\\", \\\"§artifactTypePlural\\\":\\\"wkc-governance-workflows.default.artifactType.classification.plural\\\", \\\"artifactName\\\":\\\"Confidential\\\"} \",\"task_instruction\":\" {\\\"id\\\":\\\"wkc-governance-workflows.autoPublish.authoring.taskInstructions.IMPORT.many\\\", \\\"defaultMessage\\\":\\\"§<USERID:{userId}> imported these {artifactTypePlural}. Review each {artifactType} and then publish or delete all of them together.\\\", \\\"§artifactType\\\":\\\"wkc-governance-workflows.default.artifactType.classification\\\", \\\"§artifactTypePlural\\\":\\\"wkc-governance-workflows.default.artifactType.classification.plural\\\", \\\"artifactName\\\":\\\"Confidential\\\", \\\"userId\\\":\\\"RID:1000330999&gt\\\"} \",\"task_definition_key\":\"authoring\",\"task_category\":\"authoring\",\"workflow_template_id\":\"autoPublish:1:e89aa47d-fbf2-11ee-9527-0a580a820487\",\"form_properties\":[{\"id\":\"action\",\"type\":\"enum\",\"readable\":false,\"writable\":true,\"required\":true,\"enum_values\":[{\"id\":\"#publish\",\"name\":\"{\\\"id\\\":\\\"wkc-governance-workflows.autoPublish.authoring.actionButton.publish\\\", \\\"defaultMessage\\\":\\\"Publish\\\"}\"},{\"id\":\"?discard\",\"name\":\"{\\\"id\\\":\\\"wkc-governance-workflows.autoPublish.authoring.actionButton.discard\\\", \\\"defaultMessage\\\":\\\"Delete draft\\\"}\"}],\"cpd_type\":\"action\"},{\"id\":\"comment\",\"type\":\"string\",\"readable\":false,\"writable\":true,\"required\":false,\"cpd_type\":\"comment\"}],\"candidate_users\":[\"1000330999\"]}}],\"workflow_state\":\"Not started\"}}\n",
      "Task Id =  c22ea12a-fc90-11ee-9527-0a580a820487\n",
      "Publish Successful, Code =  204\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Data Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a. Add new Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- Create Data Classes from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-data-classes.csv\" \n",
    "urlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b. Publish the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Business Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a. Add new Business Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- Create Business Terms from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-business-terms.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b. Publish the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Reference Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Add new Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Create Reference Data from CSV----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'access_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m IMPORT_CSV_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgovernance-reference-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m urlSuffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/v3/governance_artifact_types/reference_data/import?merge_option=all\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m access_token}\n\u001b[1;32m      6\u001b[0m files \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: (IMPORT_CSV_FILE, \u001b[38;5;28mopen\u001b[39m(IMPORT_CSV_FILE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/csv\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[1;32m      8\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(myconn\u001b[38;5;241m.\u001b[39murlRequest(urlSuffix), headers\u001b[38;5;241m=\u001b[39mheaders, files\u001b[38;5;241m=\u001b[39mfiles)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'access_token' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"---- Create Reference Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-reference-data.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Publish the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load Department Lookup Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.a Add the lookup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---- Load Department Lookup Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-reference-department.csv\"\n",
    "\n",
    "artifact_id = None\n",
    "version_id = None\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'\n",
    "headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    for i in r.json()[\"resources\"] :\n",
    "        if i[\"name\"] == \"Department Lookup\" :\n",
    "            artifact_id = i[\"artifact_id\"]\n",
    "            version_id = i[\"version_id\"]\n",
    "            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n",
    "            break\n",
    "else :\n",
    "    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "if artifact_id is None or version_id is None:\n",
    "    print(\"Department Lookup not found\")\n",
    "else :    \n",
    "    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n",
    "    headers = {\"Authorization\" : \"Bearer \" + access_token }\n",
    "    import_parameters = {\n",
    "        \"artifact_id_mode\": False,\n",
    "        \"code\": \"DEPARTMENT_CODE\",\n",
    "        \"first_row_header\": True,\n",
    "        \"import_relationships_only\": False,\n",
    "        \"skip_workflow_if_possible\": False, \n",
    "        \"trim_white_spaces\": True,\n",
    "        \"value\": \"DEPARTMENT_EN\",\n",
    "        \"value_conflicts\": \"OVERWRITE\" \n",
    "    }\n",
    "    files={\n",
    "        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n",
    "        'import_parameters' : (None, str(import_parameters))   \n",
    "    }\n",
    "    \n",
    "    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "    if r.status_code == 202 :\n",
    "        import_id = r.json()[\"import_info\"][\"import_id\"]\n",
    "        print(f\"----- Import process started: {import_id} ----- \")\n",
    "        print(\"----- Entering wait loop ------\")\n",
    "        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n",
    "        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "        workflow_id = r.json()[\"workflow_id\"]\n",
    "\n",
    "        while True :\n",
    "            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "            if r.status_code != 200 :\n",
    "                print(\"Error with the request. Code: \", r.status_code)\n",
    "                print(r.text)\n",
    "                break\n",
    "            status = r.json()[\"import_info\"][\"import_state\"]\n",
    "            if status != \"IN_PROGRESS\" :\n",
    "                break\n",
    "            else :\n",
    "                print (\"Import in progess, please wait\")\n",
    "                time.sleep(5)\n",
    "        print(\"Import finished. Status = \", r.status_code)\n",
    "    else :\n",
    "        print(\"Error with the request. Code: \", r.status_code)\n",
    "        print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.b. Publish the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load Position Lookup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.a. Add the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load Position Lookup Data from CSV----\n",
      "artifact_id =  d5ee35ab-7084-4dd1-861f-b2724ae16eb9  version_id =  5e9dc25d-6719-4057-a4ac-f2c87549b101_0\n",
      "{\n",
      "  \"import_info\": {\n",
      "    \"href\": \"/v4/reference_data_sets/d5ee35ab-7084-4dd1-861f-b2724ae16eb9/versions/5e9dc25d-6719-4057-a4ac-f2c87549b101_0/value_imports/ab3efcfe-71a0-4897-be4a-7faa2b14012f\",\n",
      "    \"import_id\": \"ab3efcfe-71a0-4897-be4a-7faa2b14012f\",\n",
      "    \"import_state\": \"IN_PROGRESS\",\n",
      "    \"import_message\": \"Import in progress\",\n",
      "    \"values_count\": 45,\n",
      "    \"values_processed\": 45,\n",
      "    \"values_skipped\": 0,\n",
      "    \"values_inserted\": 0,\n",
      "    \"rels_values_processed\": 0,\n",
      "    \"rels_processed\": 0,\n",
      "    \"rels_inserted\": 0,\n",
      "    \"rels_deleted\": 0,\n",
      "    \"rels_skipped\": 0,\n",
      "    \"warnings_count\": 0,\n",
      "    \"started_by\": \"1000330999\",\n",
      "    \"start_time\": \"2024-04-17T13:45:07.808Z\",\n",
      "    \"warnings_limit\": 100\n",
      "  },\n",
      "  \"href\": \"/v4/reference_data_sets/d5ee35ab-7084-4dd1-861f-b2724ae16eb9/versions/5e9dc25d-6719-4057-a4ac-f2c87549b101_0\",\n",
      "  \"artifact_id\": \"d5ee35ab-7084-4dd1-861f-b2724ae16eb9\",\n",
      "  \"version_id\": \"eee7e13e-c72b-42e6-974a-73e77b52b849\",\n",
      "  \"workflow_id\": \"b3aa8002-fcc0-11ee-9527-0a580a820487\",\n",
      "  \"global_id\": \"726304da-9fc7-4003-b816-746a4d5ce56c_d5ee35ab-7084-4dd1-861f-b2724ae16eb9\",\n",
      "  \"entity_type\": \"reference_data\"\n",
      "}\n",
      "----- Import process started: ab3efcfe-71a0-4897-be4a-7faa2b14012f ----- \n",
      "----- Entering wait loop ------\n",
      "Import finished. Status =  200\n",
      "{\n",
      "  \"import_info\": {\n",
      "    \"href\": \"/v4/reference_data_sets/d5ee35ab-7084-4dd1-861f-b2724ae16eb9/versions/5e9dc25d-6719-4057-a4ac-f2c87549b101_0/value_imports/ab3efcfe-71a0-4897-be4a-7faa2b14012f/ab3efcfe-71a0-4897-be4a-7faa2b14012f\",\n",
      "    \"import_id\": \"ab3efcfe-71a0-4897-be4a-7faa2b14012f\",\n",
      "    \"import_state\": \"SUCCEEDED\",\n",
      "    \"import_message\": \"Import Completed\",\n",
      "    \"values_count\": 45,\n",
      "    \"values_processed\": 45,\n",
      "    \"values_skipped\": 0,\n",
      "    \"values_inserted\": 45,\n",
      "    \"rels_values_processed\": 45,\n",
      "    \"rels_processed\": 0,\n",
      "    \"rels_inserted\": 0,\n",
      "    \"rels_deleted\": 0,\n",
      "    \"rels_skipped\": 0,\n",
      "    \"warnings_count\": 0,\n",
      "    \"started_by\": \"1000330999\",\n",
      "    \"start_time\": \"2024-04-17T13:45:07.808Z\",\n",
      "    \"warnings_limit\": 100\n",
      "  },\n",
      "  \"href\": \"/v4/reference_data_sets/d5ee35ab-7084-4dd1-861f-b2724ae16eb9/versions/5e9dc25d-6719-4057-a4ac-f2c87549b101_0/ab3efcfe-71a0-4897-be4a-7faa2b14012f\",\n",
      "  \"artifact_id\": \"d5ee35ab-7084-4dd1-861f-b2724ae16eb9\",\n",
      "  \"version_id\": \"5e9dc25d-6719-4057-a4ac-f2c87549b101_0\",\n",
      "  \"global_id\": \"726304da-9fc7-4003-b816-746a4d5ce56c_d5ee35ab-7084-4dd1-861f-b2724ae16eb9\",\n",
      "  \"entity_type\": \"reference_data\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"---- Load Position Lookup Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"governance-reference-position.csv\"\n",
    "\n",
    "artifact_id = None\n",
    "version_id = None\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'\n",
    "headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    for i in r.json()[\"resources\"] :\n",
    "        if i[\"name\"] == \"Position Lookup\" :\n",
    "            artifact_id = i[\"artifact_id\"]\n",
    "            version_id = i[\"version_id\"]\n",
    "            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n",
    "            break\n",
    "else :\n",
    "    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "if artifact_id is None or version_id is None:\n",
    "    print(\"Position Lookup not found\")\n",
    "else :    \n",
    "    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n",
    "    headers = {\"Authorization\" : \"Bearer \" + access_token }\n",
    "    import_parameters = {\n",
    "        \"artifact_id_mode\": False,\n",
    "        \"code\": \"POSITION_CODE\",\n",
    "        \"first_row_header\": True,\n",
    "        \"import_relationships_only\": False,\n",
    "        \"skip_workflow_if_possible\": False, \n",
    "        \"trim_white_spaces\": True,\n",
    "        \"value\": \"POSITION_EN\",\n",
    "        \"value_conflicts\": \"OVERWRITE\" \n",
    "    }\n",
    "    files={\n",
    "        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n",
    "        'import_parameters' : (None, str(import_parameters))   \n",
    "    }\n",
    "    \n",
    "    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "    if r.status_code == 202 :\n",
    "        import_id = r.json()[\"import_info\"][\"import_id\"]\n",
    "        print(f\"----- Import process started: {import_id} ----- \")\n",
    "        print(\"----- Entering wait loop ------\")\n",
    "        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n",
    "        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "        workflow_id = r.json()[\"workflow_id\"]\n",
    "\n",
    "        while True :\n",
    "            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "            if r.status_code != 200 :\n",
    "                print(\"Error with the request. Code: \", r.status_code)\n",
    "                print(r.text)\n",
    "                break\n",
    "            status = r.json()[\"import_info\"][\"import_state\"]\n",
    "            if status != \"IN_PROGRESS\" :\n",
    "                break\n",
    "            else :\n",
    "                print (\"Import in progess, please wait\")\n",
    "                time.sleep(5)\n",
    "        print(\"Import finished. Status = \", r.status_code)\n",
    "    else :\n",
    "        print(\"Error with the request. Code: \", r.status_code)\n",
    "        print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.b. Publish the draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
