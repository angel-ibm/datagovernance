{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#data-governance-tutorial-api","title":"Data Governance Tutorial - API","text":"<p>Welcome to data governance with IBM Cloud Pak for Data and IBM Knowledge Catalog.</p> <p>Presentation video. Choose the tab of your preferred language. </p> DeutschEspa\u00f1olEnglish <p></p> <p></p> <p></p> <p>Unmute the sound with the control bar at the bottom of the video (speaker icon)</p> <p>Data Governance:  https://www.ibm.com/products/cloud-pak-for-data/governance</p> <p>IBM Knowledge Catalog: https://www.ibm.com/products/knowledge-catalog</p>"},{"location":"basic/","title":"Basics","text":""},{"location":"basic/#basic-tasks","title":"Basic Tasks","text":"<p>A graphical user interface is the way to get familiar with a vendor's data governance product line and even use it in production. However, the use of a programatic API like the Watson Data API, which is a quite normal REST interface,  can be very useful in the following situations:</p> <ul> <li>Automate common tasks or even one-time activities like backups and migrations</li> <li>Interface with other tools and programs that may use the REST interface</li> <li>Perform repetitive tasks involving a considerable amount of data</li> </ul> <p>In general, a REST API exposes a series of endpoints (URLs), each one intended to perform an individual task that can be customized providing the parameters prescribed in the documentation. This tutorial aims at easing the learning curve when trying to implement programs (actually, python scripts) that handle the calls to the most common methds of the Watson Data API, which involve not only to choose the right endpoint but also the expressing with the right syntax the intended parameters.</p> <p>The Watson Data API does not need to be installed explictitly. Anyone having access to a running Cloud Pak for Data deployment can use it, provided that he/she has the adequate privileges and, of course, the syntax is correct.</p> <p>Before attempting to use any of the methods of the API, we need to get authenticated by Cloud Pak for Data. We usually type our userid/password in the user interface, which is good for humans, but it is certainly not optimized for programs. That is why the Watson Data API make use of the so called \"Bearer Tokens\" to assert our identity every time we try to perform a task.</p>"},{"location":"basic/#authentication-the-bearer-token","title":"Authentication -  The Bearer Token","text":"<p>Obtaining a bearer token and refreshing it when it expires are mandatory pre-requisites for all calls. The full process is explaned here </p> <p>Bearer tokens issued by the IBM Cloud expire after one hour. Remember to obtain a new one regularly</p> <p>The following snippets will generate a bearer token derived from the API key</p> PythonBash token.py<pre><code>import json\nimport requests\n\n# Read the API key from the file downloaded in the IBM Cloud\nf = open(\"wkcapikey.json\")\ndata = json.load(f)\napikey = data[\"apikey\"]\n\n# Get a bearer token with the API key\nurl = \"https://iam.cloud.ibm.com/identity/token\"\nheaders = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\ndata = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n\nr = requests.post(url, headers=headers, data=data)\naccess_token = r.json()[\"access_token\"]\n</code></pre> token.sh<pre><code>#!/bin/bash \n\n# Read the API key from the file downloaded in the IBM Cloud\n\nFILEAPIKEY=wkcapikey.json\napikey=$(jq -r .apikey $FILEAPIKEY)\n\n# Get a bearer token with the API key\n\nurl=\"https://iam.cloud.ibm.com/identity/token\"\nheader=\" -H Content-Type: application/x-www-form-urlencoded \"\nflags=\" -s -X POST\"\ndata=\" -d grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\"\ndata=${data}${apikey}\n\ntoken=$( \\\n    curl $flags $url $header $data \\\n    | jq -r .access_token)\n</code></pre>"},{"location":"basic/#common-tasks","title":"Common Tasks","text":"<p>Provided that the issuer of the call has been granted with the proper rights, the following tasks can be easily performed using the API and will be exercised in this chapter:</p> REST Call Description <code>GET /v2/projects</code> List the projects available for the issuer of the call in Cloud Pak for Data <code>GET /v2/catalogs</code> List the catalogs available in Cloud Pak for Data. Not only the list, but the specific information of one of them can be retrieved using parameter filtering <code>POST /v3/search</code> Search for any piece of information by using queries in Lucene or Elasticsearch syntax <code>GET /v3/governance_artifact_types/export</code> Export the full set of artifacts to a ZIP file <code>GET /v3/governance_artifact_types/{artifact_type}/export</code> Export just one kind of artifacts to a CSV file. The business terms (<code>glossary_term</code>) will be shown ind this chapter <code>POST /v3/governance_artifact_types/import</code> Import all artifacts from a ZIP file. <code>POST /v3/governance_artifact_types/{artifact_type}/import</code> Import just one kind of artifacts from a CSV file. The business terms (<code>glossary_term</code>) will be shown in this chapter"},{"location":"basic/#projects-and-catalogs","title":"Projects and Catalogs","text":"<p>Inspect existing artifacts</p> PythonBash projects_and_catalogs.py<pre><code>    # With the bearer token, we can issue requests \n\n    print(\"---- Projects ----\")\n\n    url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v2/projects\"\n    headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    r = requests.get(url, headers=headers)\n    for i in r.json()[\"resources\"] :\n        print(i[\"entity\"][\"name\"])\n\n    print(\"---- Catalogs ----\")\n\n    url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs\"\n    headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    r = requests.get(url, headers=headers)\n    for i in r.json()[\"catalogs\"] :\n        print(i[\"entity\"][\"name\"])\n</code></pre> projects_and_catalogs.sh<pre><code>    # With the bearer token, we can issue requests \n\n    echo \"---- Projects ----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/projects\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\"  \n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" | jq  '.resources[].entity.name'\n\n    echo \"---- Catalogs ----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\" \n\n    # all catalogs\n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" | jq '.catalogs[].entity.name'     \n</code></pre>"},{"location":"basic/#one-specific-item","title":"One specific item","text":"<p>Get only one artifact</p> PythonBash only_one_item.py<pre><code>        print(\"---- The catalog called Catalog-Angel ----\")\n        mycatalog=\"Catalog-Angel\"\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs?name=\" + mycatalog\n\n        r = requests.get(url, headers=headers)\n        for i in r.json()[\"catalogs\"] :\n            print(i[\"metadata\"][\"guid\"])\n            print(i[\"entity\"][\"name\"])\n\n        mycatalog_id = i[\"metadata\"][\"guid\"] \n        print(f\"---- The catalog with id = {mycatalog_id}  ----\")\n\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs/\" + mycatalog_id\n        r = requests.get(url, headers=headers)\n        print(r.json()[\"entity\"][\"name\"])\n</code></pre> only_one_item.sh<pre><code>        #  my catalog by name\n        CATALOG_NAME=\"Catalog-Angel\"\n        params=\"?name=${CATALOG_NAME}\"\n\n        echo ---- The catalog named $CATALOG_NAME ----\n\n        curl $flags $url$params $header \\\n           -H \"Authorization: Bearer ${token}\" | jq '.catalogs[] | [ .metadata.guid , .entity.name ]'\n\n        #  my catalog by id\n        mycatalog_id=$(curl $flags $url$params $header \\\n           -H \"Authorization: Bearer ${token}\" | jq -r '.catalogs[] | .metadata.guid ')\n\n        echo --- The catalog with id=$mycatalog_id ----\n\n        url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v2/catalogs/$mycatalog_id\"\n\n    curl $flags $url $header \\\n   -H \"Authorization: Bearer ${token}\" | jq '.entity.name '     \n</code></pre> <p>Retrieve specfic assets</p> <p>The following snippets will obtain  </p> Pythonbash projects_and_catalogs.py<pre><code>        import json\n        import requests\n\n        # Read the API key from the file downloaded in the IBM Cloud\n        f = open(\"wkcapikey.json\")\n        data = json.load(f)\n        apikey = data[\"apikey\"]\n\n        # Get a bearer token with the API key\n        url = \"https://iam.cloud.ibm.com/identity/token\"\n        headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n\n        r = requests.post(url, headers=headers, data=data)\n        access_token = r.json()[\"access_token\"]\n\n        # With the bearer token, we can issue requests \n\n        print(\"---- All Categories ----\")\n\n        url = \"https://api.eu-de.dataplatform.cloud.ibm.com/v3/search\"\n        headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n        data = '''{\n            \"query\": {\n                \"match\": {\n                    \"metadata.artifact_type\": \"category\"\n                }\n            }\n        }'''\n\n        r = requests.post(url, headers=headers, data=data)\n\n        for i in r.json()[\"rows\"] :\n            print(i[\"metadata\"][\"name\"])\n\n        print(\"---- Business Terms of MotoGP ----\")\n\n        data='''{\n            \"_source\":[ \"metadata.name\", \"metadata.description\"],\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\n                            \"match\": {\n                                \"metadata.artifact_type\": \"glossary_term\"\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"categories.primary_category_name\": \"MotoGP\"\n                            }\n                        }\n                    ]\n                }\n            }\n        }'''\n\n        r = requests.post(url, headers=headers, data=data)\n        for i in r.json()[\"rows\"] :\n            print(i[\"metadata\"][\"name\"], end=\": \")\n            print(i[\"metadata\"][\"description\"], end=\"\\n\\n\")\n</code></pre> <p>Too many fancy quotes, braces, escape backslashes\u2026 better in python</p>"},{"location":"basic/#export-artifacts","title":"Export Artifacts","text":"<p>Store all artifacts in a zip file</p> bashpython export_all.sh<pre><code>    echo \"---- Export ----\"\n\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/export?include_custom_attribute_definitions=true\"\n    flags=\" -s -X GET\"\n    header=\"-H content-type: application/json\"  \n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" \\\n       -o governance_artifacts.zip\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p> <p>Change this to export only the business terms in a csv file</p> Bashpython export.sh<pre><code>    echo \"---- Export only the business terms to CSV----\"\n\n    url=\"https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/glossary_term/export\"\n\n    curl $flags $url $header \\\n       -H \"Authorization: Bearer ${token}\" \\\n       -o business_terms.csv\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p>"},{"location":"basic/#import-artifacts","title":"Import Artifacts","text":"<p>Import Business Terms</p> Bashpython import_business_terms.sh<pre><code>    echo \"---- Import business terms from CSV----\"\n\n    flags=' -X POST '\n    url=' https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/glossary_term/import?merge_option=all '\n    header=' -H Content-Type:multipart/form-data '\n\n    curl $flags $url $header \\\n       -H \"Authorization:Bearer ${token}\" \\\n       -F \"file=@\\\"mybusiness_terms.csv\\\";type=text/csv\"\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p> <p>Import all governance artifacts from a ZIP file</p> Bashpython import_all.sh<pre><code>    echo \"---- Import all artifact from a ZIP File ----\"\n\n    flags=' -s -X POST '\n    url=' https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/import?merge_option=specified '\n    header=' -H Content-Type:multipart/form-data '\n    import_process=$(curl $flags $url $header \\\n       -H \"Authorization:Bearer ${token}\" \\\n       -F \"file=@\\\"governance_artifacts.zip\\\"\"  \\\n       | jq -r .process_id)\n\n    echo \"----- Import process started: $import_process ----- \"\n\n    flags=' -s -X GET '\n    url=\" https://api.eu-de.dataplatform.cloud.ibm.com/v3/governance_artifact_types/import/status/${import_process} \"\n\n    while true\n    do\n        import_status=$(curl $flags $url \\\n        -H \"Authorization: Bearer ${token}\" \\\n        | jq -r .status)\n\n\n        if [ $import_status != \"IN_PROGRESS\" ]\n        then\n            break\n        fi\n\n        echo Import job in process. Please wait...\n        sleep 5\n    done\n\n    curl $flags $url \\\n        -H \"Authorization: Bearer ${token}\"\n</code></pre> <p>I think all these import/export will be more often run in shell scripts\u2026 better to use bash for it</p>"},{"location":"pox/","title":"Level 4 PoX","text":""},{"location":"pox/#level-4-pox-knowledge-catalog","title":"Level 4 PoX - Knowledge Catalog","text":"<p>IBM Data and AI Live Demos is a web site that contains detailed instructions, code repositories, scripts, and various other tools for IBM sellers and business partners for learning and getting hands-on practice with Cloud Pak for Data solutions.</p> <p> https://cp4d-outcomes.techzone.ibm.com</p> <p>From all available resources, this documentation enhances the  Level 4 PoX - Knowledge Catalog section by adding Jupyter notebooks to automate some tasks that may not be executed with the graphical user interface in real life projects like migrations, massive initial setup of governance artifacts, etc.</p> <p>Several notebooks are provided here \"as-is\" for learning purposes. They are intended to be adapted to  the specific needs of a project, a demonstration, etc. They can be reviewed in this web site, the cells can be copied-and-pasted individually and the full code can be downloaded for convenience. Additionally, they can be executed online using, for example, the github codespaces feature if the adequate Cloud Pak for Data environment has been provisioned.</p>"},{"location":"pox/#how-to-download-a-notebook-to-your-laptop","title":"How to download a notebook to your laptop","text":"<p>The blue sections below include a link for displaying the notebooks. If you click on them, the notebook will be shown and you will see the download icon on the top right corner. Just klick on it to start downloading.</p> <p></p> <p>Note that this link is just for reviewing and downloading the code. If you want to execute the notebook, you will need to store and run it from your laptop or in a proper enviroment like the one we will describe in the next section.</p>"},{"location":"pox/#how-to-run-a-notebook-online","title":"How to run a notebook online","text":"<p>The magenta sections below contain a link for accesing the notebooks in the original github repository. If you click on it, you will be directed to the right location on github.com. Go to the right corner, unfold the menu and select <code>github.dev</code> as shown in this picture:</p> <p></p> <p>Now, go again to the top right corner, click on <code>select kernel</code> and then click on <code>open in a codespace</code>:</p> <p></p> <p>Then, click on one of the options:</p> <p></p> <p>Finally, you can execute the notebook as it would be local in your laptop. Note that we incorporate this feature to the documentation just for convenience, only to try small things quickly.  </p>"},{"location":"pox/#sample-notebooks","title":"Sample Notebooks","text":""},{"location":"pox/#define-the-business-vocabulary","title":"Define the Business Vocabulary","text":"<p>This notebook automates all the tasks of the section <code>Define the Business Vocabulary</code></p> <p></p> <p>Display (and download) the notebook</p> <p>Click to display and download the notebook</p> <p>Access the original notebook (and execute it online) </p> <p>Click to access the actual file and execute it online</p>"},{"location":"pox/#define-the-rules-and-policies","title":"Define the Rules and Policies","text":"<p>This notebook automates all the tasks of the section <code>Define Rules and Policies</code></p> <p></p> <p>Display (and download) the notebook</p> <p>Click to display the notebook</p> <p>Access the original notebook (and execute it online) </p> <p>Click to access the actual file and execute it online</p>"},{"location":"pox/Define_Business_Vocabulary/","title":"Define Business Vocabulary","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nimport requests # type: ignore\nimport time\n\nLOCAL_DIR_PREFIX = \"\"\n\n# uncomment the next line if you cloned the repository and the notebook will run locally on your laptop\n# LOCAL_DIR_PREFIX = \"../../\"\n\n# uncomment the next line if hardcoding the credentials on the code, global class variables\nFILE_CREDENTIALS = \"\"\n\n# uncomment the next two lines if using the file ikcapikey.json for storing credentials\n# FILE_CREDENTIALS = \"python/ikcapikey.json\"\n# FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS\n\nclass credentials :\n\n    file_credentials = \"\"\n    url_server = \"https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"\n    username = \"admin\"\n    apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"\n    access_token = \"\"\n\n    def __init__(self, file_credentials):\n\n        if file_credentials != \"\" :\n            try :\n                with open(file_credentials) as f :\n                    data = json.load(f)\n                    self.url_server = data[\"url_server\"]\n                    self.username = data[\"username\"]\n                    self.apikey = data[\"api_key\"]\n                    self.file_credentials = file_credentials\n            except :\n                print(\"Error with the file \", file_credentials)\n    \n   \n    def urlRequest(self, urlSuffix):\n        return self.url_server + urlSuffix\n\n    def get_bearer_token(self):\n        \n        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n        # url = \"https://iam.cloud.ibm.com/identity/token\"\n        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n        # r = requests.post(url, headers=headers, data=data)\n        # access_token = r.json()[\"access_token\"]\n\n        # Get a bearer token with the API key - Cloud Pak for Data Software\n        urlSuffix = \"/icp4d-api/v1/authorize\"\n        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n\n        if r.status_code != 200:\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        else :\n            try:\n                self.access_token = r.json()[\"token\"]\n            except KeyError:\n                print(\"Error with the token. Code: \", r.status_code)\n                print(\"Hint: check the credentials file \", self.file_credentials)\n                print(r.text)\n                \n            return self.access_token\n\nmyconn = credentials(FILE_CREDENTIALS)\naccess_token = myconn.get_bearer_token()\n</pre> import json import requests # type: ignore import time  LOCAL_DIR_PREFIX = \"\"  # uncomment the next line if you cloned the repository and the notebook will run locally on your laptop # LOCAL_DIR_PREFIX = \"../../\"  # uncomment the next line if hardcoding the credentials on the code, global class variables FILE_CREDENTIALS = \"\"  # uncomment the next two lines if using the file ikcapikey.json for storing credentials # FILE_CREDENTIALS = \"python/ikcapikey.json\" # FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS  class credentials :      file_credentials = \"\"     url_server = \"https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"     username = \"admin\"     apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"     access_token = \"\"      def __init__(self, file_credentials):          if file_credentials != \"\" :             try :                 with open(file_credentials) as f :                     data = json.load(f)                     self.url_server = data[\"url_server\"]                     self.username = data[\"username\"]                     self.apikey = data[\"api_key\"]                     self.file_credentials = file_credentials             except :                 print(\"Error with the file \", file_credentials)              def urlRequest(self, urlSuffix):         return self.url_server + urlSuffix      def get_bearer_token(self):                  # Get a bearer token with the API key - Cloud Pak for Data SaaS         # url = \"https://iam.cloud.ibm.com/identity/token\"         # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}         # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey         # r = requests.post(url, headers=headers, data=data)         # access_token = r.json()[\"access_token\"]          # Get a bearer token with the API key - Cloud Pak for Data Software         urlSuffix = \"/icp4d-api/v1/authorize\"         headers = {'Accept': 'application/json', 'Content-type': 'application/json'}         data = {\"username\" : self.username, \"api_key\" : self.apikey}         r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))          if r.status_code != 200:             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         else :             try:                 self.access_token = r.json()[\"token\"]             except KeyError:                 print(\"Error with the token. Code: \", r.status_code)                 print(\"Hint: check the credentials file \", self.file_credentials)                 print(r.text)                              return self.access_token  myconn = credentials(FILE_CREDENTIALS) access_token = myconn.get_bearer_token()  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Categories from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-categories.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/category/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n</pre> print(\"---- Import Categories from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-categories.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/category/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Update Classifications from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-classifications.csv\" \nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Update Classifications from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-classifications.csv\"  IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Data Classes from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-data-classes.csv\" \nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Data Classes from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-data-classes.csv\"  IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Business Terms from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-business-terms.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Business Terms from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-business-terms.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Create Reference Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-data.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n            break\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Create Reference Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-data.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)  elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}      while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)             break         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"] In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Load Department Lookup Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-department.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nartifact_id = None\nversion_id = None\nurlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200'\nheaders = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nif r.status_code == 200 :\n    for i in r.json()[\"resources\"] :\n        if i[\"name\"] == \"Department Lookup\" :\n            artifact_id = i[\"artifact_id\"]\n            version_id = i[\"version_id\"]\n            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n            break\nelse :\n    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n    print(r.text)\n\nif artifact_id is None or version_id is None:\n    print(\"Department Lookup not found\")\nelse :    \n    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n    headers = {\"Authorization\" : \"Bearer \" + access_token }\n    import_parameters = {\n        \"artifact_id_mode\": False,\n        \"code\": \"DEPARTMENT_CODE\",\n        \"first_row_header\": True,\n        \"import_relationships_only\": False,\n        \"skip_workflow_if_possible\": False, \n        \"trim_white_spaces\": True,\n        \"value\": \"DEPARTMENT_EN\",\n        \"value_conflicts\": \"OVERWRITE\" \n    }\n    files={\n        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n        'import_parameters' : (None, str(import_parameters))   \n    }\n    \n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\n    if r.status_code == 202 :\n        import_id = r.json()[\"import_info\"][\"import_id\"]\n        print(f\"----- Import process started: {import_id} ----- \")\n        print(\"----- Entering wait loop ------\")\n        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n        workflow_id = r.json()[\"workflow_id\"]\n\n        while True :\n            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n            if r.status_code != 200 :\n                print(\"Error with the request. Code: \", r.status_code)\n                print(r.text)\n                break\n            status = r.json()[\"import_info\"][\"import_state\"]\n            if status != \"IN_PROGRESS\" :\n                break\n            else :\n                print (\"Import in progess, please wait\")\n                time.sleep(5)\n        print(\"Import finished. Status = \", r.status_code)\n    else :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> print(\"---- Load Department Lookup Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-department.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  artifact_id = None version_id = None urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200' headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}  r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  if r.status_code == 200 :     for i in r.json()[\"resources\"] :         if i[\"name\"] == \"Department Lookup\" :             artifact_id = i[\"artifact_id\"]             version_id = i[\"version_id\"]             print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)             break else :     print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)     print(r.text)  if artifact_id is None or version_id is None:     print(\"Department Lookup not found\") else :         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'     headers = {\"Authorization\" : \"Bearer \" + access_token }     import_parameters = {         \"artifact_id_mode\": False,         \"code\": \"DEPARTMENT_CODE\",         \"first_row_header\": True,         \"import_relationships_only\": False,         \"skip_workflow_if_possible\": False,          \"trim_white_spaces\": True,         \"value\": \"DEPARTMENT_EN\",         \"value_conflicts\": \"OVERWRITE\"      }     files={         'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),         'import_parameters' : (None, str(import_parameters))        }          r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)      if r.status_code == 202 :         import_id = r.json()[\"import_info\"][\"import_id\"]         print(f\"----- Import process started: {import_id} ----- \")         print(\"----- Entering wait loop ------\")         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id         headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}          workflow_id = r.json()[\"workflow_id\"]          while True :             r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)             if r.status_code != 200 :                 print(\"Error with the request. Code: \", r.status_code)                 print(r.text)                 break             status = r.json()[\"import_info\"][\"import_state\"]             if status != \"IN_PROGRESS\" :                 break             else :                 print (\"Import in progess, please wait\")                 time.sleep(5)         print(\"Import finished. Status = \", r.status_code)     else :         print(\"Error with the request. Code: \", r.status_code)         print(r.text) In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre>print(\"---- Load Position Lookup Data from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-reference-position.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nartifact_id = None\nversion_id = None\nurlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200'\nheaders = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nif r.status_code == 200 :\n    for i in r.json()[\"resources\"] :\n        if i[\"name\"] == \"Position Lookup\" :\n            artifact_id = i[\"artifact_id\"]\n            version_id = i[\"version_id\"]\n            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n            break\nelse :\n    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n    print(r.text)\n\nif artifact_id is None or version_id is None:\n    print(\"Position Lookup not found\")\nelse :    \n    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n    headers = {\"Authorization\" : \"Bearer \" + access_token }\n    import_parameters = {\n        \"artifact_id_mode\": False,\n        \"code\": \"POSITION_CODE\",\n        \"first_row_header\": True,\n        \"import_relationships_only\": False,\n        \"skip_workflow_if_possible\": False, \n        \"trim_white_spaces\": True,\n        \"value\": \"POSITION_EN\",\n        \"value_conflicts\": \"OVERWRITE\" \n    }\n    files={\n        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n        'import_parameters' : (None, str(import_parameters))   \n    }\n    \n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\n    if r.status_code == 202 :\n        import_id = r.json()[\"import_info\"][\"import_id\"]\n        print(f\"----- Import process started: {import_id} ----- \")\n        print(\"----- Entering wait loop ------\")\n        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n\n        workflow_id = r.json()[\"workflow_id\"]\n\n        while True :\n            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n            if r.status_code != 200 :\n                print(\"Error with the request. Code: \", r.status_code)\n                print(r.text)\n                break\n            status = r.json()[\"import_info\"][\"import_state\"]\n            if status != \"IN_PROGRESS\" :\n                break\n            else :\n                print (\"Import in progess, please wait\")\n                time.sleep(5)\n        print(\"Import finished. Status = \", r.status_code)\n    else :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre>  print(\"---- Load Position Lookup Data from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-reference-position.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  artifact_id = None version_id = None urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&amp;limit=200' headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}  r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  if r.status_code == 200 :     for i in r.json()[\"resources\"] :         if i[\"name\"] == \"Position Lookup\" :             artifact_id = i[\"artifact_id\"]             version_id = i[\"version_id\"]             print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)             break else :     print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)     print(r.text)  if artifact_id is None or version_id is None:     print(\"Position Lookup not found\") else :         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'     headers = {\"Authorization\" : \"Bearer \" + access_token }     import_parameters = {         \"artifact_id_mode\": False,         \"code\": \"POSITION_CODE\",         \"first_row_header\": True,         \"import_relationships_only\": False,         \"skip_workflow_if_possible\": False,          \"trim_white_spaces\": True,         \"value\": \"POSITION_EN\",         \"value_conflicts\": \"OVERWRITE\"      }     files={         'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),         'import_parameters' : (None, str(import_parameters))        }          r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)      if r.status_code == 202 :         import_id = r.json()[\"import_info\"][\"import_id\"]         print(f\"----- Import process started: {import_id} ----- \")         print(\"----- Entering wait loop ------\")         urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id         headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}          workflow_id = r.json()[\"workflow_id\"]          while True :             r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)             if r.status_code != 200 :                 print(\"Error with the request. Code: \", r.status_code)                 print(r.text)                 break             status = r.json()[\"import_info\"][\"import_state\"]             if status != \"IN_PROGRESS\" :                 break             else :                 print (\"Import in progess, please wait\")                 time.sleep(5)         print(\"Import finished. Status = \", r.status_code)     else :         print(\"Error with the request. Code: \", r.status_code)         print(r.text) In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)"},{"location":"pox/Define_Business_Vocabulary/#define-business-vocabulary","title":"Define Business Vocabulary\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#authorisation","title":"Authorisation\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#define-the-business-vocabulary","title":"Define the Business Vocabulary\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#1-create-categories","title":"1. Create Categories\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2-update-classifications","title":"2. Update Classifications\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2a-change-the-definitions","title":"2.a. Change the definitions\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#2b-publish-the-definitions","title":"2.b. Publish the definitions\u00b6","text":"<p>Before executing this cell, you may want to check the \"Task Inbox\" in CloudPak for Data if you are not sure about what will be published</p>"},{"location":"pox/Define_Business_Vocabulary/#3-create-data-classes","title":"3. Create Data Classes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#3a-add-new-data-classes","title":"3.a. Add new Data Classes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#3b-publish-the-changes","title":"3.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4-create-business-terms","title":"4. Create Business Terms\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4a-add-new-business-terms","title":"4.a. Add new Business Terms\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#4b-publish-the-changes","title":"4.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5-create-reference-data","title":"5. Create Reference Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5a-add-new-reference-data","title":"5.a Add new Reference Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#5b-publish-the-changes","title":"5.b Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6-load-department-lookup-data","title":"6. Load Department Lookup Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6a-add-the-lookup-data","title":"6.a Add the lookup data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#6b-publish-the-changes","title":"6.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7-load-position-lookup-data","title":"7. Load Position Lookup Data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7a-add-the-new-data","title":"7.a. Add the new data\u00b6","text":""},{"location":"pox/Define_Business_Vocabulary/#7b-publish-the-draft","title":"7.b. Publish the draft\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/","title":"Define Rules and Policies","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nimport requests # type: ignore\nimport time\n\nLOCAL_DIR_PREFIX = \"\"\n\n# uncomment the next line if you cloned the repository and the notebook will run locally on your laptop\n# LOCAL_DIR_PREFIX = \"../../\"\n\n# uncomment the next line if hardcoding the credentials on the code, global class variables\nFILE_CREDENTIALS = \"\"\n\n# uncomment the next two lines if using the file ikcapikey.json for storing credentials\n# FILE_CREDENTIALS = \"python/ikcapikey.json\"\n# FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS\n\nclass credentials :\n\n    file_credentials = \"\"\n    url_server = \" https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"\n    username = \"admin\"\n    apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"\n    access_token = \"\"\n\n    def __init__(self, file_credentials):\n\n        if file_credentials != \"\" :\n            try :\n                with open(file_credentials) as f :\n                    data = json.load(f)\n                    self.url_server = data[\"url_server\"]\n                    self.username = data[\"username\"]\n                    self.apikey = data[\"api_key\"]\n                    self.file_credentials = file_credentials\n            except :\n                print(\"Error with the file \", file_credentials)\n    \n   \n    def urlRequest(self, urlSuffix):\n        return self.url_server + urlSuffix\n\n    def get_bearer_token(self):\n        \n        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n        # url = \"https://iam.cloud.ibm.com/identity/token\"\n        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey\n        # r = requests.post(url, headers=headers, data=data)\n        # access_token = r.json()[\"access_token\"]\n\n        # Get a bearer token with the API key - Cloud Pak for Data Software\n        urlSuffix = \"/icp4d-api/v1/authorize\"\n        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n\n        if r.status_code != 200:\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        else :\n            try:\n                self.access_token = r.json()[\"token\"]\n            except KeyError:\n                print(\"Error with the token. Code: \", r.status_code)\n                print(\"Hint: check the credentials file \", self.file_credentials)\n                print(r.text)\n                \n            return self.access_token\n\nmyconn = credentials(FILE_CREDENTIALS)\naccess_token = myconn.get_bearer_token()\n</pre> import json import requests # type: ignore import time  LOCAL_DIR_PREFIX = \"\"  # uncomment the next line if you cloned the repository and the notebook will run locally on your laptop # LOCAL_DIR_PREFIX = \"../../\"  # uncomment the next line if hardcoding the credentials on the code, global class variables FILE_CREDENTIALS = \"\"  # uncomment the next two lines if using the file ikcapikey.json for storing credentials # FILE_CREDENTIALS = \"python/ikcapikey.json\" # FILE_CREDENTIALS = LOCAL_DIR_PREFIX + FILE_CREDENTIALS  class credentials :      file_credentials = \"\"     url_server = \" https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"     username = \"admin\"     apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"     access_token = \"\"      def __init__(self, file_credentials):          if file_credentials != \"\" :             try :                 with open(file_credentials) as f :                     data = json.load(f)                     self.url_server = data[\"url_server\"]                     self.username = data[\"username\"]                     self.apikey = data[\"api_key\"]                     self.file_credentials = file_credentials             except :                 print(\"Error with the file \", file_credentials)              def urlRequest(self, urlSuffix):         return self.url_server + urlSuffix      def get_bearer_token(self):                  # Get a bearer token with the API key - Cloud Pak for Data SaaS         # url = \"https://iam.cloud.ibm.com/identity/token\"         # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}         # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&amp;apikey=\" + apikey         # r = requests.post(url, headers=headers, data=data)         # access_token = r.json()[\"access_token\"]          # Get a bearer token with the API key - Cloud Pak for Data Software         urlSuffix = \"/icp4d-api/v1/authorize\"         headers = {'Accept': 'application/json', 'Content-type': 'application/json'}         data = {\"username\" : self.username, \"api_key\" : self.apikey}         r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))          if r.status_code != 200:             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         else :             try:                 self.access_token = r.json()[\"token\"]             except KeyError:                 print(\"Error with the token. Code: \", r.status_code)                 print(\"Hint: check the credentials file \", self.file_credentials)                 print(r.text)                              return self.access_token  myconn = credentials(FILE_CREDENTIALS) access_token = myconn.get_bearer_token()  <p>1.a. Add new rules</p> In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Governance Rules from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-rules.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/rule/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Import Governance Rules from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-rules.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/rule/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text) In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Email Address\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"Email Address\"\ndata_class_name_for_search = \"Email ?Address\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nelse : \n\n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {\n                \"name\": \"Protect Email Address\",\n                \"description\": \"Protect all email addresses using the data privacy advanced masking method.\",\n                \"governance_type_id\": \"Access\",\n                \"trigger\": [\n                    \"$Asset.InferredClassification\",\n                    \"CONTAINS\",\n                    [\n                        '$' + global_id\n                    ]\n                ],\n                \"action\": {\n                    \"name\": \"Transform\",\n                    \"subaction\": {\n                        \"name\": \"pseudonymizeTerms\",\n                        \"parameters\": [\n                            {\n                                \"name\": \"term_name\",\n                                \"value\": global_id\n                            },\n                            {\n                                \"name\": \"maskingType\",\n                                \"value\": \"Partial\"\n                            },\n                            {\n                                \"name\": \"maskingProcessor\",\n                                \"value\": \"RepeatableFormatFabrication\"\n                            },\n                            {\n                                \"name\": \"preserveFormat\",\n                                \"value\": \"true\"\n                            },\n                            {\n                                \"name\": \"maskingOptions\",\n                                \"value\": [\n                                    {\n                                        \"name\": \"User name\",\n                                        \"value\": \"Generate user name\"\n                                    },\n                                    {\n                                        \"name\": \"Domain name\",\n                                        \"value\": \"Original\"\n                                    }\n                                ]\n                            }\n                        ]\n                    }\n                },\n                \"state\": \"active\"\n            }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Email Address\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"Email Address\" data_class_name_for_search = \"Email ?Address\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  else :       for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                 \"name\": \"Protect Email Address\",                 \"description\": \"Protect all email addresses using the data privacy advanced masking method.\",                 \"governance_type_id\": \"Access\",                 \"trigger\": [                     \"$Asset.InferredClassification\",                     \"CONTAINS\",                     [                         '$' + global_id                     ]                 ],                 \"action\": {                     \"name\": \"Transform\",                     \"subaction\": {                         \"name\": \"pseudonymizeTerms\",                         \"parameters\": [                             {                                 \"name\": \"term_name\",                                 \"value\": global_id                             },                             {                                 \"name\": \"maskingType\",                                 \"value\": \"Partial\"                             },                             {                                 \"name\": \"maskingProcessor\",                                 \"value\": \"RepeatableFormatFabrication\"                             },                             {                                 \"name\": \"preserveFormat\",                                 \"value\": \"true\"                             },                             {                                 \"name\": \"maskingOptions\",                                 \"value\": [                                     {                                         \"name\": \"User name\",                                         \"value\": \"Generate user name\"                                     },                                     {                                         \"name\": \"Domain name\",                                         \"value\": \"Original\"                                     }                                 ]                             }                         ]                     }                 },                 \"state\": \"active\"             }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Phone Number\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"Phone Number\"\ndata_class_name_for_search = \"Phone ?Number\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\nelse : \n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {            \n            \"name\": \"Protect Phone Numbers\",\n            \"description\": \"Protect all phone numbers using the redaction data privacy masking method.\",\n            \"governance_type_id\": \"Access\",\n            \"trigger\": [\n                \"$Asset.InferredClassification\",\n                \"CONTAINS\",\n                [\n                    '$' + global_id\n                ]\n            ],\n            \"action\": {\n                \"name\": \"Transform\",\n                \"subaction\": {\n                    \"name\": \"redactTerms\",\n                    \"parameters\": [\n                        {\n                            \"name\": \"term_name\",\n                            \"value\": global_id\n                        },\n                        {\n                            \"name\": \"maskingChar\",\n                            \"value\": \"X\"\n                        },\n                        {\n                            \"name\": \"maskingType\",\n                            \"value\": \"Full\"\n                        },\n                        {\n                            \"name\": \"preserveFormat\",\n                            \"value\": \"true\"\n                        }\n                    ]\n                }\n            },\n            \"state\": \"active\"\n        }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Phone Number\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"Phone Number\" data_class_name_for_search = \"Phone ?Number\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text) else :      for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                         \"name\": \"Protect Phone Numbers\",             \"description\": \"Protect all phone numbers using the redaction data privacy masking method.\",             \"governance_type_id\": \"Access\",             \"trigger\": [                 \"$Asset.InferredClassification\",                 \"CONTAINS\",                 [                     '$' + global_id                 ]             ],             \"action\": {                 \"name\": \"Transform\",                 \"subaction\": {                     \"name\": \"redactTerms\",                     \"parameters\": [                         {                             \"name\": \"term_name\",                             \"value\": global_id                         },                         {                             \"name\": \"maskingChar\",                             \"value\": \"X\"                         },                         {                             \"name\": \"maskingType\",                             \"value\": \"Full\"                         },                         {                             \"name\": \"preserveFormat\",                             \"value\": \"true\"                         }                     ]                 }             },             \"state\": \"active\"         }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre># Look for the global id of the data class called \"Phone Number\"\n\nurlSuffix = \"/v3/search\"\nheaders = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\ndata_class_name = \"US Social Security Number\"\ndata_class_name_for_search = \"US ?Social ?Security ?Number\" # The lucene syntax requires a ? after the space\ndata={\n    \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\n                    \"match\": {\n                        \"metadata.artifact_type\": \"data_class\"\n                    }\n                },\n                {\n                    \"match\": {\n                        \"metadata.name\" : data_class_name_for_search\n                    }\n                }\n            ]\n        }\n    }\n}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n\nif r.status_code != 200 :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\nelse : \n    for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves\n        if i[\"metadata\"][\"name\"] == data_class_name :\n            global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]\n\n    # With this global id, create the rule\n\n    urlSuffix='/v3/enforcement/rules'\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    data = {            \n           \"name\": \"Protect US Social Security Numbers\",\n            \"description\": \"Protect all US social security numbers using the data privacy advanced masking method.\",\n            \"governance_type_id\": \"Access\",\n            \"trigger\": [\n                \"$Asset.InferredClassification\",\n                \"CONTAINS\",\n                [\n                    \"$\" +  global_id\n                ]\n            ],\n            \"action\": {\n                \"name\": \"Transform\",\n                \"subaction\": {\n                    \"name\": \"pseudonymizeTerms\",\n                    \"parameters\": [\n                        {\n                            \"name\": \"term_name\",\n                            \"value\": global_id\n                        },\n                        {\n                            \"name\": \"maskingType\",\n                            \"value\": \"Full\"\n                        },\n                        {\n                            \"name\": \"maskingProcessor\",\n                            \"value\": \"FormatPreservingTokenization\"\n                        },\n                        {\n                            \"name\": \"preserveFormat\",\n                            \"value\": \"true\"\n                        }\n                    ]\n                }\n            },\n            \"state\": \"active\"\n        }\n\n    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)\n    \n    if r.status_code not in (200, 201) :\n        print(\"Error with the request. Code: \", r.status_code)\n        print(r.text)\n</pre> # Look for the global id of the data class called \"Phone Number\"  urlSuffix = \"/v3/search\" headers = {\"content-type\" : \"application/json\", \"Authorization\" : \"Bearer \" + access_token} data_class_name = \"US Social Security Number\" data_class_name_for_search = \"US ?Social ?Security ?Number\" # The lucene syntax requires a ? after the space data={     \"_source\":[ \"metadata.name\", \"entity.artifacts.global_id\"],     \"query\": {         \"bool\": {             \"must\": [                 {                     \"match\": {                         \"metadata.artifact_type\": \"data_class\"                     }                 },                 {                     \"match\": {                         \"metadata.name\" : data_class_name_for_search                     }                 }             ]         }     } } r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)  if r.status_code != 200 :     print(\"Error with the request. Code: \", r.status_code)     print(r.text) else :      for i in r.json()[\"rows\"] :  # Never trust the lucene syntax... who knows what it retrieves         if i[\"metadata\"][\"name\"] == data_class_name :             global_id = i[\"entity\"][\"artifacts\"][\"global_id\"]      # With this global id, create the rule      urlSuffix='/v3/enforcement/rules'     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     data = {                        \"name\": \"Protect US Social Security Numbers\",             \"description\": \"Protect all US social security numbers using the data privacy advanced masking method.\",             \"governance_type_id\": \"Access\",             \"trigger\": [                 \"$Asset.InferredClassification\",                 \"CONTAINS\",                 [                     \"$\" +  global_id                 ]             ],             \"action\": {                 \"name\": \"Transform\",                 \"subaction\": {                     \"name\": \"pseudonymizeTerms\",                     \"parameters\": [                         {                             \"name\": \"term_name\",                             \"value\": global_id                         },                         {                             \"name\": \"maskingType\",                             \"value\": \"Full\"                         },                         {                             \"name\": \"maskingProcessor\",                             \"value\": \"FormatPreservingTokenization\"                         },                         {                             \"name\": \"preserveFormat\",                             \"value\": \"true\"                         }                     ]                 }             },             \"state\": \"active\"         }      r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=data)          if r.status_code not in (200, 201) :         print(\"Error with the request. Code: \", r.status_code)         print(r.text)  In\u00a0[\u00a0]: Copied! <pre>print(\"---- Import Policies from CSV----\")\n\nIMPORT_CSV_FILE = \"artifacts/governance-policies.csv\"\nIMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE\n\nurlSuffix='/v3/governance_artifact_types/policy/import?merge_option=all'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nfiles = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n\nif r.status_code == 200 :\n    status = r.json()[\"status\"]\n    print(\"Import finished. Status = \", status)\n    print(r.text)\nelif r.status_code == 202 :\n    process_id = r.json()[\"process_id\"]\n    print(f\"----- Import process started: {process_id} ----- \")\n    print(\"----- Entering wait loop ------\")\n    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n    while True :\n        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n        if r.status_code != 200 :\n            print(\"Error with the request. Code: \", r.status_code)\n            print(r.text)\n        status = r.json()[\"status\"]\n        if status != \"IN_PROGRESS\" :\n            break\n        else :\n            print (\"Import in progess, please wait\")\n            time.sleep(5)\nelse :\n    print(\"Error with the request. Code: \", r.status_code)\n    print(r.text)\n\nworkflow_id = r.json()[\"workflow_id\"]\n</pre> print(\"---- Import Policies from CSV----\")  IMPORT_CSV_FILE = \"artifacts/governance-policies.csv\" IMPORT_CSV_FILE = LOCAL_DIR_PREFIX + IMPORT_CSV_FILE  urlSuffix='/v3/governance_artifact_types/policy/import?merge_option=all' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}  r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)  if r.status_code == 200 :     status = r.json()[\"status\"]     print(\"Import finished. Status = \", status)     print(r.text) elif r.status_code == 202 :     process_id = r.json()[\"process_id\"]     print(f\"----- Import process started: {process_id} ----- \")     print(\"----- Entering wait loop ------\")     urlSuffix='/v3/governance_artifact_types/import/status/' + process_id     headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}     while True :         r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)         if r.status_code != 200 :             print(\"Error with the request. Code: \", r.status_code)             print(r.text)         status = r.json()[\"status\"]         if status != \"IN_PROGRESS\" :             break         else :             print (\"Import in progess, please wait\")             time.sleep(5) else :     print(\"Error with the request. Code: \", r.status_code)     print(r.text)  workflow_id = r.json()[\"workflow_id\"]  In\u00a0[\u00a0]: Copied! <pre>urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\nr = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n\nuser_tasks = r.json()[\"entity\"][\"user_tasks\"]\nfor i in user_tasks :\n    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n        task_id = i[\"metadata\"][\"task_id\"]\n\nurlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\nheaders = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\npayload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\nr = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n\nif r.status_code == 202 or r.status_code == 204 :\n    print(\"Publish Successful, Code = \", r.status_code)\nelse :\n    print(\"Error in publishing artifacts, Code = \", r.status_code)\n    print(r.text)\n</pre> urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)  user_tasks = r.json()[\"entity\"][\"user_tasks\"] for i in user_tasks :     if i[\"metadata\"][\"workflow_id\"] == workflow_id :         task_id = i[\"metadata\"][\"task_id\"]  urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions' headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token} payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]} r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)  if r.status_code == 202 or r.status_code == 204 :     print(\"Publish Successful, Code = \", r.status_code) else :     print(\"Error in publishing artifacts, Code = \", r.status_code)     print(r.text)"},{"location":"pox/Define_Rules_and_Policies/#define-rules-and-policies","title":"Define Rules and Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#authorisation","title":"Authorisation\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#define-rules-and-policies","title":"Define Rules and Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#1-create-governance-rules","title":"1. Create Governance Rules\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#1b-publish-the-changes","title":"1.b. Publish the changes\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#2-create-email-protection-rule","title":"2. Create Email Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#3-create-phone-protection-rule","title":"3. Create Phone Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#4-create-us-ssn-protection-rule","title":"4. Create US SSN Protection Rule\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5-create-policies","title":"5. Create Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5a-add-new-policies","title":"5.a. Add new Policies\u00b6","text":""},{"location":"pox/Define_Rules_and_Policies/#5b-publish-the-changes","title":"5.b. Publish the changes\u00b6","text":""}]}