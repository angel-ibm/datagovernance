{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Business Vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests # type: ignore\n",
    "import time\n",
    "\n",
    "class credentials :\n",
    "\n",
    "    file_credentials = \"\"\n",
    "    url_server = \"https://cpd-cpd.apps.6645c6d6ca5b92001e29286f.cloud.techzone.ibm.com\"\n",
    "    username = \"admin\"\n",
    "    apikey = \"SfpLD0yMQFh4xpdOrgPuTK9AdBtEVEqF1gK2HSlw\"\n",
    "    access_token = \"\"\n",
    "\n",
    "    def __init__(self, file_credentials):\n",
    "\n",
    "        if file_credentials != \"\" :\n",
    "            try :\n",
    "                with open(file_credentials) as f :\n",
    "                    data = json.load(f)\n",
    "                    self.url_server = data[\"url_server\"]\n",
    "                    self.username = data[\"username\"]\n",
    "                    self.apikey = data[\"api_key\"]\n",
    "                    self.file_credentials = file_credentials\n",
    "            except :\n",
    "                print(\"Error with the file \", file_credentials)\n",
    "    \n",
    "   \n",
    "    def urlRequest(self, urlSuffix):\n",
    "        return self.url_server + urlSuffix\n",
    "\n",
    "    def get_bearer_token(self):\n",
    "        \n",
    "        # Get a bearer token with the API key - Cloud Pak for Data SaaS\n",
    "        # url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "        # headers = {\"Content-Type\" : \"application/x-www-form-urlencoded\"}\n",
    "        # data = \"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey=\" + apikey\n",
    "        # r = requests.post(url, headers=headers, data=data)\n",
    "        # access_token = r.json()[\"access_token\"]\n",
    "\n",
    "        # Get a bearer token with the API key - Cloud Pak for Data Software\n",
    "        urlSuffix = \"/icp4d-api/v1/authorize\"\n",
    "        headers = {'Accept': 'application/json', 'Content-type': 'application/json'}\n",
    "        data = {\"username\" : self.username, \"api_key\" : self.apikey}\n",
    "        r = requests.post(self.urlRequest(urlSuffix), headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "        else :\n",
    "            try:\n",
    "                self.access_token = r.json()[\"token\"]\n",
    "            except KeyError:\n",
    "                print(\"Error with the token. Code: \", r.status_code)\n",
    "                print(\"Hint: check the credentials file \", self.file_credentials)\n",
    "                print(r.text)\n",
    "                \n",
    "            return self.access_token\n",
    "    \n",
    "# FILE_CREDENTIALS = \"../../python/ikcapikey.json\"\n",
    "FILE_CREDENTIALS = \"\"\n",
    "\n",
    "myconn = credentials(FILE_CREDENTIALS)\n",
    "access_token = myconn.get_bearer_token()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Business Vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Categories\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Import Categories from CSV----\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.../../artifacts/governance-categories.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m urlSuffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/v3/governance_artifact_types/category/import?merge_option=all\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m access_token}\n\u001b[0;32m----> 6\u001b[0m files \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: (IMPORT_CSV_FILE, \u001b[38;5;28mopen\u001b[39m(IMPORT_CSV_FILE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/csv\u001b[39m\u001b[38;5;124m'\u001b[39m)}\n\u001b[1;32m      8\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(myconn\u001b[38;5;241m.\u001b[39murlRequest(urlSuffix), headers\u001b[38;5;241m=\u001b[39mheaders, files\u001b[38;5;241m=\u001b[39mfiles)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m :\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.../../artifacts/governance-categories.csv'"
     ]
    }
   ],
   "source": [
    "print(\"---- Import Categories from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../../artifacts/governance-categories.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/category/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Update Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a. Change the definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Update Classifications from CSV----\n",
      "Import finished. Status =  SUCCEEDED\n",
      "{\n",
      "  \"process_id\": \"4ea91cdb-6c88-47ec-80aa-eff1c1083d23\",\n",
      "  \"status\": \"SUCCEEDED\",\n",
      "  \"step_number\": 16,\n",
      "  \"total_steps\": 16,\n",
      "  \"step_message\": \"Artifacts import finished\",\n",
      "  \"creation_time\": \"2024-05-21T08:40:15.150Z\",\n",
      "  \"completion_time\": \"2024-05-21T08:40:25.870Z\",\n",
      "  \"tenant_id\": \"999\",\n",
      "  \"workflow_id\": \"c063eec0-174d-11ef-8e75-0a580a820497\",\n",
      "  \"heartbeat_time\": \"2024-05-21T08:40:25.870Z\",\n",
      "  \"messages\": {\n",
      "    \"resources\": [],\n",
      "    \"offset\": 0,\n",
      "    \"set_uri\": false,\n",
      "    \"limit\": 200,\n",
      "    \"count\": 0\n",
      "  },\n",
      "  \"operations_count\": {\n",
      "    \"relationship\": {\n",
      "      \"IMPORT_CREATE\": 8\n",
      "    },\n",
      "    \"classification\": {\n",
      "      \"IMPORT_CREATE\": 0,\n",
      "      \"IMPORT_MODIFY\": 4\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Update Classifications from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-classifications.csv\" \n",
    "urlSuffix='/v3/governance_artifact_types/classification/import?merge_option=specified'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b. Publish the definitions\n",
    "\n",
    "Before executing this cell, you may want to check the \"Task Inbox\" in CloudPak for Data if you are not sure about what will be published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  204\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Data Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a. Add new Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Create Data Classes from CSV----\n",
      "Import finished. Status =  SUCCEEDED\n",
      "{\n",
      "  \"process_id\": \"4bf54fb3-a26f-45f8-9944-7840e6975852\",\n",
      "  \"status\": \"SUCCEEDED\",\n",
      "  \"step_number\": 16,\n",
      "  \"total_steps\": 16,\n",
      "  \"step_message\": \"Artifacts import finished\",\n",
      "  \"creation_time\": \"2024-05-21T08:42:08.457Z\",\n",
      "  \"completion_time\": \"2024-05-21T08:42:16.680Z\",\n",
      "  \"tenant_id\": \"999\",\n",
      "  \"workflow_id\": \"0444f2e7-174e-11ef-8e75-0a580a820497\",\n",
      "  \"heartbeat_time\": \"2024-05-21T08:42:16.680Z\",\n",
      "  \"messages\": {\n",
      "    \"resources\": [],\n",
      "    \"offset\": 0,\n",
      "    \"set_uri\": false,\n",
      "    \"limit\": 200,\n",
      "    \"count\": 0\n",
      "  },\n",
      "  \"operations_count\": {\n",
      "    \"data_class\": {\n",
      "      \"IMPORT_CREATE\": 15,\n",
      "      \"IMPORT_MODIFY\": 0\n",
      "    },\n",
      "    \"relationship\": {\n",
      "      \"IMPORT_CREATE\": 15\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Create Data Classes from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-data-classes.csv\" \n",
    "urlSuffix='/v3/governance_artifact_types/data_class/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "    print(r.text)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b. Publish the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  204\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Business Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a. Add new Business Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Create Business Terms from CSV----\n",
      "Import finished. Status =  SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Create Business Terms from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-business-terms.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/glossary_term/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b. Publish the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Reference Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Add new Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Create Reference Data from CSV----\n",
      "Import finished. Status =  SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Create Reference Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-reference-data.csv\"\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data/import?merge_option=all'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "files = {'file': (IMPORT_CSV_FILE, open(IMPORT_CSV_FILE, 'rb'), 'text/csv')}\n",
    "\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    status = r.json()[\"status\"]\n",
    "    print(\"Import finished. Status = \", status)\n",
    "\n",
    "elif r.status_code == 202 :\n",
    "    process_id = r.json()[\"process_id\"]\n",
    "    print(f\"----- Import process started: {process_id} ----- \")\n",
    "    print(\"----- Entering wait loop ------\")\n",
    "    urlSuffix='/v3/governance_artifact_types/import/status/' + process_id\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "    while True :\n",
    "        r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "        if r.status_code != 200 :\n",
    "            print(\"Error with the request. Code: \", r.status_code)\n",
    "            print(r.text)\n",
    "            break\n",
    "        status = r.json()[\"status\"]\n",
    "        if status != \"IN_PROGRESS\" :\n",
    "            break\n",
    "        else :\n",
    "            print (\"Import in progess, please wait\")\n",
    "            time.sleep(5)\n",
    "else :\n",
    "    print(\"Error with the request. Code: \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "workflow_id = r.json()[\"workflow_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Publish the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load Department Lookup Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.a Add the lookup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load Department Lookup Data from CSV----\n",
      "artifact_id =  cfd84306-3f31-4f65-8336-4a9639d05f5a  version_id =  f4ad0be7-e41e-4dbb-8c7f-59f9ef22c8b4_0\n",
      "----- Import process started: 12813349-a2cf-4011-b233-aa13575def27 ----- \n",
      "----- Entering wait loop ------\n",
      "Import finished. Status =  200\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Load Department Lookup Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-reference-department.csv\"\n",
    "\n",
    "artifact_id = None\n",
    "version_id = None\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'\n",
    "headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    for i in r.json()[\"resources\"] :\n",
    "        if i[\"name\"] == \"Department Lookup\" :\n",
    "            artifact_id = i[\"artifact_id\"]\n",
    "            version_id = i[\"version_id\"]\n",
    "            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n",
    "            break\n",
    "else :\n",
    "    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "if artifact_id is None or version_id is None:\n",
    "    print(\"Department Lookup not found\")\n",
    "else :    \n",
    "    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n",
    "    headers = {\"Authorization\" : \"Bearer \" + access_token }\n",
    "    import_parameters = {\n",
    "        \"artifact_id_mode\": False,\n",
    "        \"code\": \"DEPARTMENT_CODE\",\n",
    "        \"first_row_header\": True,\n",
    "        \"import_relationships_only\": False,\n",
    "        \"skip_workflow_if_possible\": False, \n",
    "        \"trim_white_spaces\": True,\n",
    "        \"value\": \"DEPARTMENT_EN\",\n",
    "        \"value_conflicts\": \"OVERWRITE\" \n",
    "    }\n",
    "    files={\n",
    "        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n",
    "        'import_parameters' : (None, str(import_parameters))   \n",
    "    }\n",
    "    \n",
    "    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "    if r.status_code == 202 :\n",
    "        import_id = r.json()[\"import_info\"][\"import_id\"]\n",
    "        print(f\"----- Import process started: {import_id} ----- \")\n",
    "        print(\"----- Entering wait loop ------\")\n",
    "        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n",
    "        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "        workflow_id = r.json()[\"workflow_id\"]\n",
    "\n",
    "        while True :\n",
    "            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "            if r.status_code != 200 :\n",
    "                print(\"Error with the request. Code: \", r.status_code)\n",
    "                print(r.text)\n",
    "                break\n",
    "            status = r.json()[\"import_info\"][\"import_state\"]\n",
    "            if status != \"IN_PROGRESS\" :\n",
    "                break\n",
    "            else :\n",
    "                print (\"Import in progess, please wait\")\n",
    "                time.sleep(5)\n",
    "        print(\"Import finished. Status = \", r.status_code)\n",
    "    else :\n",
    "        print(\"Error with the request. Code: \", r.status_code)\n",
    "        print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.b. Publish the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load Position Lookup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.a. Add the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load Position Lookup Data from CSV----\n",
      "artifact_id =  26051dfb-c580-4a74-87d4-4b605525a61b  version_id =  43c25345-8ecf-4bb8-981c-9b442ecb618a_0\n",
      "----- Import process started: 98a08973-af4b-42e2-9eec-a9c87768f256 ----- \n",
      "----- Entering wait loop ------\n",
      "Import finished. Status =  200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"---- Load Position Lookup Data from CSV----\")\n",
    "\n",
    "IMPORT_CSV_FILE = \"../governance-reference-position.csv\"\n",
    "\n",
    "artifact_id = None\n",
    "version_id = None\n",
    "urlSuffix='/v3/governance_artifact_types/reference_data?workflow_status=published&limit=200'\n",
    "headers = {\"accept\": \"application/json\" ,\"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "if r.status_code == 200 :\n",
    "    for i in r.json()[\"resources\"] :\n",
    "        if i[\"name\"] == \"Position Lookup\" :\n",
    "            artifact_id = i[\"artifact_id\"]\n",
    "            version_id = i[\"version_id\"]\n",
    "            print(\"artifact_id = \", artifact_id, \" version_id = \" , version_id)\n",
    "            break\n",
    "else :\n",
    "    print(\"Error in retrieving reference data artifacts, Code = \", r.status_code)\n",
    "    print(r.text)\n",
    "\n",
    "if artifact_id is None or version_id is None:\n",
    "    print(\"Position Lookup not found\")\n",
    "else :    \n",
    "    urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports'\n",
    "    headers = {\"Authorization\" : \"Bearer \" + access_token }\n",
    "    import_parameters = {\n",
    "        \"artifact_id_mode\": False,\n",
    "        \"code\": \"POSITION_CODE\",\n",
    "        \"first_row_header\": True,\n",
    "        \"import_relationships_only\": False,\n",
    "        \"skip_workflow_if_possible\": False, \n",
    "        \"trim_white_spaces\": True,\n",
    "        \"value\": \"POSITION_EN\",\n",
    "        \"value_conflicts\": \"OVERWRITE\" \n",
    "    }\n",
    "    files={\n",
    "        'import_csv_file'   : ('import_csv_file', open(IMPORT_CSV_FILE,'rb') ),\n",
    "        'import_parameters' : (None, str(import_parameters))   \n",
    "    }\n",
    "    \n",
    "    r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, files=files)\n",
    "\n",
    "    if r.status_code == 202 :\n",
    "        import_id = r.json()[\"import_info\"][\"import_id\"]\n",
    "        print(f\"----- Import process started: {import_id} ----- \")\n",
    "        print(\"----- Entering wait loop ------\")\n",
    "        urlSuffix='/v4/reference_data_sets/' + artifact_id + '/versions/' + version_id + '/value_imports/' + import_id\n",
    "        headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "\n",
    "        workflow_id = r.json()[\"workflow_id\"]\n",
    "\n",
    "        while True :\n",
    "            r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "            if r.status_code != 200 :\n",
    "                print(\"Error with the request. Code: \", r.status_code)\n",
    "                print(r.text)\n",
    "                break\n",
    "            status = r.json()[\"import_info\"][\"import_state\"]\n",
    "            if status != \"IN_PROGRESS\" :\n",
    "                break\n",
    "            else :\n",
    "                print (\"Import in progess, please wait\")\n",
    "                time.sleep(5)\n",
    "        print(\"Import finished. Status = \", r.status_code)\n",
    "    else :\n",
    "        print(\"Error with the request. Code: \", r.status_code)\n",
    "        print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.b. Publish the draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Successful, Code =  202\n"
     ]
    }
   ],
   "source": [
    "urlSuffix='/v3/workflows/' + workflow_id + '?include_user_tasks=true'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "r = requests.get(myconn.urlRequest(urlSuffix), headers=headers)\n",
    "\n",
    "user_tasks = r.json()[\"entity\"][\"user_tasks\"]\n",
    "for i in user_tasks :\n",
    "    if i[\"metadata\"][\"workflow_id\"] == workflow_id :\n",
    "        task_id = i[\"metadata\"][\"task_id\"]\n",
    "\n",
    "urlSuffix='/v3/workflow_user_tasks/' + task_id + '/actions'\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\" : \"Bearer \" + access_token}\n",
    "payload = {'action': 'complete', 'form_properties': [{'id': 'action', 'value': '#publish'}]}\n",
    "r = requests.post(myconn.urlRequest(urlSuffix), headers=headers, json=payload)\n",
    "\n",
    "if r.status_code == 202 or r.status_code == 204 :\n",
    "    print(\"Publish Successful, Code = \", r.status_code)\n",
    "else :\n",
    "    print(\"Error in publishing artifacts, Code = \", r.status_code)\n",
    "    print(r.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
